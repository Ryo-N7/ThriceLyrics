---
title: "Untitled"
author: "RN7"
date: "December 13, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Get song lyrics and metadata through webscraping!

Whether you are humming it on the way to work, bellowing out the chorus in the shower, or 
AZ lyrics >>> website i've been using for lyrics from middle school...

still going strong

and website is structured well for this kind of thing...

```{r packages, eval = FALSE}
library(stringr)     # dealing with strings
library(lubridate)   # dealing with date variables
library(purrr)       # mapping functions over vectors
library(dplyr)       # data manipulation and tidying
library(rvest)       # web-scraping
```

# scrape song names from Artist Page:

I wrote a brief intro on web-scraping in another (article)[] but we'll go over them again here. 
(**Using the Mozilla Firefox browser**):

1. Press **F12** or **Ctrl + Shift + C** to open up the *Inspector Tool* on the page you want to extract data from.
2. Click on the left-most button on the pop-up bar.
3. Hover your mouse over the element of the page you want to extract. The console will scroll to the exact html code that you are pointing at.
4. Right-click on the highlighted html code, click on 'Copy' in the menu, then click on 'CSS Selector'.
5. Then Paste into R and place it inside the `html_nodes()` function as above within brackets.
6. Select only the element with the information you want with square brackets `.[[some_number]]`.
7. Finally we specifically extract the text using `html_text()`. Other options include `html_table()` for tables.

There are other ways to do this such as using the *Selector Gadget* ([Tutorial](https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/)).

In the context of our little project, we also have an intermediate step of using `html_attr()` to specify that we are looking for the `href` URL link and not the anchor text (the clickable text that appears on the page). Following the web-scraping portion we do a bit of wrangling to get the full URL link for each of the songs to show up correctly. 

```{r}
url <- "https://www.azlyrics.com/t/thrice.html"


url %>% 
  read_html() %>% 
  html_nodes("#listAlbum > a") %>% 
  html_attr("href") %>% 
  as_tibble() %>% 
  na.omit() %>% # cut out rows with NA values
  mutate(value = str_replace(value, "..", ""),
         value = paste0("https://azlyrics.com", value))

```

How about for a different band? All you have to do is change the target url to the band/musician of your preference:

```{r}
url2 <- "https://www.azlyrics.com/s/sum41.html"


url2 %>% 
  read_html() %>% 
  html_nodes("#listAlbum > a") %>% 
  html_attr("href") %>% 
  as_tibble() %>% 
  na.omit() %>% 
  mutate(value = str_replace(value, "..", "")) %>% 
  mutate(value = paste0("https://azlyrics.com", value))


```

We will save what we did above as `url_link` as this tibble will hold all the URL links for each song by the band/artist of your choice.

```{r}
url <- "https://www.azlyrics.com/t/thrice.html"

url_link <- url %>% 
  read_html() %>% 
  html_nodes("#listAlbum > a") %>% 
  html_attr("href") %>% 
  as_tibble() %>% 
  na.omit() %>% 
  mutate(value = str_replace(value, "..", "")) %>% 
  mutate(value = paste0("https://azlyrics.com", value)) %>% as.matrix()

head(url_link, 6)
class(url_link)

save(url_link, file = "url_link.Rdata")

```

Now that we have the links for each song, we can use `map_df()` to create a data frame with the lyrics for each song!

In the code below, the title of each song is the 2nd element (`.[[2]]`) in the `html_node "b"` while the data for the lyrics are contained in the node `"div.col-xs-12:nth-child(2) > div:nth-child(8)"`. Finally, pass `html_text()` to scrape it as text. One very important rule for web-scraping, especially for multiple page queries, is to include `Sys.sleep()` function so that you don't bombard the website of interest with a barrage of queries all at once! You can specify how long to pause in the function, in this case the value is a random sample between 0 to 2 seconds in increments of 0.5 seconds. You can specify longer or shorter durations by considering how many queries you are sending, that's up to you. 

```{r map_df lyrics}

thrice_lyrics <- map_df(url_link, function(i) {
  
  pg <- read_html(i) 
  
  tibble(
    title = pg %>%  
      html_nodes("b") %>% 
      .[[2]] %>% 
      html_text(),
    
    lyrics = pg %>% 
      html_nodes("div.col-xs-12:nth-child(2) > div:nth-child(8)") %>% 
      html_text(),
    
    Sys.sleep(sample(seq(0, 2, 0.5), 1)) 
  )
  
})
```


If you are doing this for a different band, just double-check to make sure the CSS selector for the nodes are the right ones! 

Remember to save your data frame so you don't have to web scrape each time you come back to your project:

```{r save lyrics}

write.csv(thrice_lyrics, "~/R_materials/ThriceLyrics/thrice_lyrics.csv")

thrice_lyrics <- read.csv("~/R_materials/ThriceLyrics/thrice_lyrics.csv", 
                             stringsAsFactors = FALSE)

glimpse(thrice_lyrics)

```


## clean the strings:

Our scraped dataframe has a lot of artifacts in the strings that we need to clean out, a good package to use for string operations is `stringr`. 

```{r clean strings}

thrice_lyrics <- thrice_lyrics %>% 
  mutate(title = str_replace_all(title, pattern = "\"", "")) %>% 
  mutate(lyrics = str_replace_all(lyrics, pattern = "\r", ""),
         lyrics = str_replace_all(lyrics, pattern = "\n", " "),
         lyrics = trimws(lyrics, "both")) %>% 
  select(-X)

glimpse(thrice_lyrics)

```

Our lyrics data set is ready so now we go on to scraping the meta data, this incldues information such as song length, song number, etc. 

## Wikipedia metadata 

For the purposes of this article we are just going for really basic meta data so we are going to look at Wikipedia for song length and song number.

Trying out different CSS Selectors and different `html_*()` functions we can grab different parts of the album html node on Thrice's Wikipedia page:

```{r}
url <- "https://en.wikipedia.org/wiki/Thrice"

# attribute (title): album titles
url %>% 
  read_html() %>% 
  html_nodes(".mw-parser-output > ul:nth-child(68) > li > i > a") %>% 
  html_attr("title")

```

```{r}
# text: album titles with year
url %>% 
  read_html() %>% 
  html_nodes(".mw-parser-output > ul:nth-child(68) > li") %>% 
  html_text("#text")
```

```{r}
# attribute (href): grab the corresponding html link to each of the albums
url %>% 
  read_html() %>% 
  html_nodes(".mw-parser-output > ul:nth-child(68) > li > i > a") %>% 
  html_attr("href")
```

What we need is, just like for the lyrics data, the URL link to each of the albums and so we specify "href" in `html_attr()`.

With this knowledge in hand, let's create the list for the links to each album:

```{r url links for metadata}
url <- "https://en.wikipedia.org/wiki/Thrice"
head_url <- "https://en.wikipedia.org"

# href
album_url <- url %>% 
  read_html() %>% 
  html_nodes(".mw-parser-output > ul:nth-child(68) > li > i > a") %>% 
  html_attr("href")


album_links <- paste0(head_url, album_url)

```

The tracklist for each album is stored in the html node ".tracklist" (in terms of its CSS Selector). Let's see what the tracklist looks like for the first Thrice album (which, in the album_links list, is the first element):

```{r}

album_links[1] %>% 
  read_html() %>% 
  html_nodes(".tracklist")

```

We get a `table` of class = "tracklist. Good, we can see that we're grabbing the right info.

Originally, code to extract the metadata table for the 2nd and 3rd element (corresponding to "The Illusion of Safety" and "The Artist in the Ambulance" albums respectively) in our album list didn't work as the corresponding Wikipedia articles stored the metadata as lists rather than tables. This also applied for Thrice's latest album (To Be Everywhere Is To Be Nowhere)[] in a slightly different way, in the end I had to go and edit all the Wikipedia data tables myself so that they were consistent and had all the information a Wikipedia "tracklist" table should have. Also, as the two parts of each The Alchemy Index album is stored in 2 different tables, I had to do that in a separate function.

With all that said, time to put it all together into one data frame, once again by using `map_df()`: 

```{r}
album_alchemy_links <- album_links[5:6]
album_links <- album_links[c(1:4,7:9)] 


thrice_metadata <- map_df(album_links, function(i) {
  
  page <- read_html(i) 
  
  meta_table <- page %>% 
      html_nodes(".tracklist") %>% 
      .[[1]] %>% 
      html_table() %>% 
      as.data.frame() 
  
  Sys.sleep(sample(seq(0, 2, 0.5), 1))
  
  meta_table <- meta_table %>% 
    mutate(album = i)
  
}) 

# alchemy index

thrice_alchemy_metadata <- map_df(album_alchemy_links, function(i) {
  
  page <- read_html(i) 
  
  meta_table <- page %>% 
      html_nodes(".tracklist") %>% 
      html_table() %>% 
      as.data.frame() 
  
  Sys.sleep(sample(seq(0, 2, 0.5), 1))
  
  meta_table <- meta_table %>% 
    mutate(album = i)
  
}) 

```

As before let's save our progress:

```{r save metadata}

write.csv(thrice_metadata, "~/R_materials/ThriceLyrics/thrice_meta_most.csv")

thrice_metadata <- read.csv("~/R_materials/ThriceLyrics/thrice_meta_most.csv", 
                            stringsAsFactors = FALSE)


write.csv(thrice_alchemy_metadata, "~/R_materials/ThriceLyrics/thrice_meta_alchemy.csv")

thrice_alchemy_metadata <- read.csv("~/R_materials/ThriceLyrics/thrice_meta_alchemy.csv", 
                            stringsAsFactors = FALSE)

```

Now let's take a look at our results!

```{r}
glimpse(thrice_metadata)

glimpse(thrice_alchemy_metadata)

glimpse(thrice_lyrics)

```

Clearly a few things need to be done:

First let's clean The Alchemy Index metadata so we can properly combine it with the main metadata dataframe:

For the peculiar case with The Alchemy Index metadata:

```{r alchemy clean}

alc_2 <- thrice_alchemy_metadata %>% 
  select(No. = No..1, Title = Title.1, Length = Length.1, album)

alc <- alc_2 %>% 
  plyr::rbind.fill(thrice_alchemy_metadata) %>% 
  select(No., Title, Length, album)

glimpse(alc)

```

Combine it with the main metadata and then clean it all in one go...

```{r bind alchemy combine metadata}

thrice_meta_complete <- alc %>% 
  plyr::rbind.fill(thrice_metadata) %>% 
    transmute(album = album %>% 
             str_replace("https://en.wikipedia.org/wiki/", "") %>% 
             str_replace_all("_", " ") %>% 
             str_replace("(Thrice album)", "") %>%
             str_replace("album", "") %>% 
             str_replace("\\(\\)", "") %>% 
             trimws(),
           track_num = `No.`,
           title = `Title` %>% stringr::str_replace_all('"', ""),
           song_length = lubridate::ms(Length)) %>% 
  select(track_num, title, album, song_length)

```

Finally we can combine the meta data and the lyrics data by a common column, the song titles!

```{r complete!}

glimpse(thrice_meta_complete)

thrice_meta_complete %>% mutate(title = tolower(title)) %>% 
  left_join(thrice_lyrics %>% mutate(title = tolower(title))) %>% glimpse()

thrice_meta_complete %>% mutate(title = tolower(title)) %>% 
  left_join(thrice_lyrics %>% mutate(title = tolower(title))) %>% 
  filter(is.na(lyrics))

thrice_complete_webscrape_df <- thrice_meta_complete %>% left_join(thrice_lyrics)   
# should join on "title" column

glimpse(thrice_webscrape_df)

```


# use tolower to make song titles as similar as possible before join
# some may need more direct adjustment ...
# some songs in `thrice` on AZ lyrics page are from EPs or bonus tracks...

We have lyrics data for 122 songs while the Wikipedia meta data picked up 133 songs... 

Some songs from AZ lyrics are from LPs and bonus tracks that I didn't web scrape for while others 

It did get a bit messy, especially cleaning the scraped data but it did most of the work and what's left is to tweak a few things and add in data for 

Why not try it out for YOUR favorite band? 

Of course, remember to check that the Wikipedia pages of each album to see that they are formatted in a uniform manner...

Some other music-themed packages:

* [SpotifyR]() by Charlie Thompson [@RCharlie]() >>> you can grab other sorts of meta data from the vast of Spotify!
* [billboard]() by M.F. Krogsholm contains datasets for the Billboard Hot 100 list from 1960 to 2016, included in this are the ranks for the given year, webscraped lyrics, and some extra spotify features such as "energy", "key", and "danceability"!
* [tuneR]() by Uew Ligges allows you to extract MFCC, wave files, and basically allows you to analyze music and speech through audio decoding!

