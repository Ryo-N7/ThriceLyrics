---
title: "Part2.r"
author: "Ryo Nakagawara"
date: "Mon Oct 09 17:39:31 2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

As always let's load the various packages we are going to be using!

```{r }
# Packages:
library(tidyverse)  # for dplyr, tidyr, ggplot2
library(tidytext)   # for separating text into words with unnest_tokens() function
library(stringr)    # for string detection, extraction, manipulation, etc.

library(gplots)     # for a certain type of plots not in ggplot2
library(ggrepel)    # for making sure labels don't overlap
library(scales)     # for fixing and tweaking the scales on graphs
library(gridExtra)  # for arranging multiple plots into a single page
```

```{r include=FALSE}
# load dataset ------------------------------------------------------------
library(lubridate)

df <- read.csv('thrice.df.csv', header=TRUE, stringsAsFactors = FALSE)

df <- df %>% 
       mutate(album = factor(album, levels = unique(album)),
              length = ms(length),
              lengthS = seconds(length))
```

- how accurate are the numLines and numWord counts?
- comparison when unnest lyrics by line and word in later section of this article!

```{r}
# Lyrics analysis ---------------------------------------------------------

df <- df %>%
  mutate(numLines = str_count(lyrics, '<br>') + 1,       # num of lines per song
         numWord = str_count(lyrics, ' ') + 1)           # num of words per song
     
```

- lyrics separated by line, "<br>" tag
- uninterested in looking at line counts so split into lines on each <br> tag 
- and then unnest each word from each "line" of lyrics

Using unnest_tokens() we need to:
1.
2.
3.
4.
5.

```{r}
wordToken <-  df %>%
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  unnest_tokens(word, line) %>% 
  mutate(wordCount = row_number()) %>%    # count of words in order of line, song, album
  select(-numLines, -numWord)             
  # take out numLines and numWord, can calculate independtly later

glimpse(wordToken)
# uncleaned unigrams containing all 'stop words' such as 'I', 'you', 'we', 'very', etc. etc.
```

Now let's count the how many times each word appears throughout the lyrics!

```{r}
countWord <- wordToken %>% count(word, sort=TRUE)
countWord  %>% head(10)
# 'the' is most common....
```

What are stop words?
Stop words are... and in the `tidytext` package they are populated from the "onix", "SMART", and "snowball" lexicons. For more information see [here]()!

```{r}
# Include stop_words ------------------------------------------------------

data("stop_words")
stop_words         # data base of "stop words" to clean unigrams.

set.seed(1)
sample_stop <- stop_words %>% sample_n(10)

sample_stop

```

You can see some of the "stop words" such as _there_, _keep_, and _everywhere_. 

Now we can create a new dataset where we filter out the "stop words"" from our `word` column in `wordToken`. 

This can be done by using anti_join() function which returns all rows from `x` (our original `wordtoken` dataset) where there are no matching values in `y` (`stop_words` dataset) on a variable with a common name across both datasets (`word`).

to use the stop words in the stop_words dataset to filter out any matching words in the "word" column of our unnested dataset that we had just created.

```{r}

wordToken2 <- wordToken %>% 
  anti_join(stop_words) %>% # Take out rows of `words` in wordToken that appear in stop_words
  select(-wordCount) %>%    # Won't need wordCount
  arrange(ID)               # Or by track_num, essentially the same thing

countWord2 <- wordToken2 %>% count(word, sort=TRUE)

countWord2 %>% head(10)

```

- with no stop words, 'eyes', ''ll', ''ve', and 'love', 'light' are most common
- interesting in -light- of the fact that sentiment-score wise Thrice songs are overtly negative...
- we'll see more of that in Part 3

```{r fig.height=5, fig.width=8, fig.align='center'}

# graph of top words (including stop words) ####
one <- countWord %>% head(10) %>% 
  ggplot(aes(reorder(word, n), n)) + 
  geom_bar(stat = "identity", fill = "darkgreen", alpha = 0.75) +
  xlab("Most common words") +
  coord_flip() +
  theme_bw() +
  theme(panel.grid.major.x = element_line(size = 1.25))

# graph of top words (no stop words) ####
two <- countWord2 %>% head(10) %>% 
  ggplot(aes(reorder(word, n), n)) + 
  geom_bar(stat = "identity", fill = "darkgreen", alpha = 0.75) +
  xlab("Most common words") +
  coord_flip() +
  theme_bw() +
  theme(panel.grid.major.x = element_line(size = 1.25))

grid.arrange(one, two)

```

You can clearly see the difference like this!

The fact that the scales for `n` are very different between the plot with "stop words" and the one without shows how individually meaningless "stop words" such as "the", "and", "to", and "a" can really disrupt our analyses. The plot without "stop words" gives us a much clearer idea of the most meaningful and common words in Thrice's lyrics!

One way to see this obscure>> disruption in action is by visualizing our data in a different way, using word clouds!

```{r fig.height=5, fig.width=8, fig.align='center'}

# WORD CLOUD (comparison stop vs. no-stop)   ####
library(wordcloud)
layout(matrix(c(1,2),1,2, byrow = TRUE))

wordcloud(words = countWord$word, freq = countWord$n, random.order = FALSE, max.words = 300, 
          colors = brewer.pal(8, "Dark2"), use.r.layout = TRUE)

# compare with wordcloud without stop_words!
wordcloud(countWord2$word, countWord2$n, random.order = FALSE, max.words = 300,
          colors = brewer.pal(8, "Dark2"), use.r.layout = TRUE)

```

With the word cloud visualization, we can really tell how the "stop words" in the left side obscures or "crowds out" all of the other more meaningful words due to the sheer amount of "the"s, "you"s, and "to"s that appear in the text.

Now that we've spread out each word into it's own row, let's take a closer look at our new datasets using our handy dplyr verbs!

```{r}
####    Explore dataset with dplyr verbs!   ####

df %>% summarise(num_songs = n()) # 103 songs in total, as each row = 1 song

```

- several ways to do this as shown in Part 1 using n_distinct() on `title`...
- the `n()` function specifically counts the # of obsv in the current group and can only be
- used inside summarize(), mutate(), and filter()

- another example of n(), usage in conjunction with group_by()
- songs per album (by order of # of songs desc) >>> did this in part 1

```{r}

df %>% group_by(album) %>% 
  summarise(num_songs = n()) %>% 
  arrange(desc(num_songs)) 

```

- words per song?

```{r}

df %>% group_by(title) %>% 
  select(title, album, numWord) %>% 
  arrange(desc(numWord))    

```

- The Weight has the most words, however the df dataset >>> (all incl. stop words)... also includes <br> though...
- need to use wordToken instead of df or wordToken2 as stop_words should be included for sum

```{r}

wordToken %>% 
  select(title, word) %>% 
  group_by(title) %>% 
  summarize(num_word = n()) %>% 
  arrange(desc(num_word))

```

More accurate measure of the total number of words per song! as counts each row which is divided into a single word from applying unnest_tokens() function to wordToken earlier!

How about words per album??

- instrumentals still count the blank as 1, but insignificant.

```{r}
# numWord by album
wordToken %>% 
  select(album, word) %>% 
  group_by(album) %>% 
  summarize(num_word = n()) %>% 
  arrange(desc(num_word)) %>% 
  ggplot(aes(reorder(album, num_word), num_word)) + 
  geom_bar(stat = "identity", fill = "darkgreen") + 
  scale_y_continuous(expand = c(0.01, 0)) +
  coord_flip() +
  theme_bw() +
  theme(axis.text.y = element_text(size = 8), axis.title.y = element_blank()) +
  ylab("Number of Words")

```

- using count of "word" in unnested wordToken 
- and original df "numWord" estimates similar enough...



#######

```{r}
# random stuff
# use str_*() functions to find and dig deeper into specific words!

df %>% 
  filter(album == "Vheissu") %>% 
  select(numWord) %>% 
  sum()

sum(str_count(df$lyrics, "<br>"))   # 2562 breaks in total

sum(str_count(df$lyrics, "light"))
wordToken$word[wordToken$word == "light"]

# counting occurences of "light" in each song 
# this looks through the "lyrics" column which as an entire song's lyrics inside and counts how many times "light" appears in the text
df %>% 
  mutate(number = str_count(lyrics, pattern = "light")) %>% 
  select(title, album, number) %>% 
  arrange(desc(number)) %>% 
  head(6)

wordToken2 %>% 
  select(word) %>% 
  str_count("light") %>% 
  sum()
# total number of times "light" appears in lyrics of song, separated by each word or tokenized dataset ...(desc. order)

# counting occurences of "light" in each song
# count each specific appearance of "light" in "word" column
# then sum up occurences by song ("title")
wordToken2 %>% 
  select(title, album, word) %>%
  mutate(light = str_count(word, "light")) %>% 
  group_by(title) %>% 
  summarize(total_light = sum(light)) %>% 
  arrange(desc(total_light)) %>% 
  head(5)
  


df %>% 
  filter(title == "The Weight") %>% 
  select(album, title, numWord, numLines)
# 421
wordToken %>% 
  filter(title == "The Weight") %>% 
  summarize(num_word = n())    # count rows, in this case: one row = one word
# 383

```

######### 



```{r}
# most frequent word in each song ####
wordToken2 %>% 
  group_by(title) %>% 
  count(word) %>% 
  arrange(desc(n))

```

- "I'll" in Black Honey >>> song about
- "image" and "invisible" in "Image of the Invisible" >>> given as shouted out during the chorus numerous
- times in the song... To consider: skewed toward phrases repeated in chorus!


```{r fig.height=5, fig.width=8, fig.align='center'}
# WordsPerNoStop
df %>%
  mutate(lyrics = str_replace_all(lyrics, '\'', ' ')) %>%
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>% 
  unnest_tokens(word, line) %>%
  anti_join(stop_words) %>%           # filter stop words
  group_by(title) %>% 
  summarize(wordcounts = n()) %>% 
  arrange(desc(wordcounts))
```

```{r fig.height=5, fig.width=8, fig.align='center'}
# Distribution of Songs by # of Words     
medianWord <- median(df$numWord)

one <- df %>% 
  ggplot(., aes(x = numWord)) +
  geom_histogram(binwidth = 10,
                 color = 'white',
                 fill = 'darkgreen') +
  geom_vline(aes(xintercept = medianWord), colour = "red", linetype = "dashed", size = 1.25) +
  scale_y_continuous(breaks = pretty_breaks(), expand = c(0, 0), limits = c(0, 12)) +
  scale_x_continuous(breaks = pretty_breaks(), expand = c(0, 0)) +
  xlab('Total # of Words') +
  ylab('# of Songs') +
  labs(title = 'Distribution of Songs by Number of Words (Est. numWord measure)', 
       subtitle = 'Dashed red line: median') + 
  theme_bw() +
  theme(panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5))
```


```{r fig.height=5, fig.width=8, fig.align='center'}
# same as above^ but should be more accurate?

# WordsPerSong
WordsPerSong <- wordToken %>%   # NOT filter stop words (included in the lyrics for the total count)
  group_by(title) %>% 
  summarize(wordcounts = n()) %>%    # each row = 1 word
  arrange(desc(wordcounts))

# Histogram of Word counts (per Song)   >>>> same as in uni-bi-tri section...
two <- WordsPerSong %>% 
  ggplot(aes(x = wordcounts)) + 
  geom_histogram(binwidth = 10, color = "white", fill = "darkgreen") +
  geom_vline(xintercept = median(WordsPerSong$wordcounts), color = "red", linetype = "dashed", size = 1.25) +
  scale_y_continuous(breaks = pretty_breaks(), expand = c(0, 0), limits = c(0, 12)) +
  scale_x_continuous(breaks = pretty_breaks(), expand = c(0, 0)) +
  xlab('Total # of Words') +
  ylab('# of Songs') +
  labs(title = 'Distribution of Songs by Number of Words (Count of each word)', 
       subtitle = 'Dashed red line: median') + 
  theme_bw() +
  theme(panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5))

# compare accuracy of numWord method used...
grid.arrange(one, two)
# crucial difference: WordsPerSong filter out instrumentals all together!
# df includes instrumentals and has counted as 1
```





```{r}
# most frequent unigrams per album: ####
# nested on albums!

word_count_nested <- wordToken2 %>% 
  group_by(album, word) %>% 
  summarize(count = n()) %>% 
  top_n(10) %>% 
  arrange(album, desc(count)) %>% 
  nest() 

```

```{r}

word_count_nested$data[[1]]

word_count_nested$data[[5]]

```

```{r}

word_count_nested <- word_count_nested %>% 
  mutate(plot = map2(data, album, 
                     ~ggplot(data = .x) +
           geom_bar(aes(reorder(word, count), count), 
                    stat = "identity", fill = "darkgreen", width = 0.65) +
           scale_y_continuous(breaks = pretty_breaks(10), limits = c(0, 22), expand = c(0, 0)) +
           ggtitle(.y) +
           labs(x = NULL, y = NULL) +
           coord_flip() +  
           theme_bw() +
           theme(axis.text.y = element_text(size = 7),
                 title = element_text(size = 7))
           ))

glimpse(word_count_nested)

```

- can now see that column "data" is series of lists that holds the top 10 words for each album (row)
- "plot" column is a series of lists that holds the plot data for each album (row)

- by selecting the specific element within the list, we can extract the plot for a certain album

```{r fig.height=5, fig.width=8, fig.align='center'}

word_count_nested$plot[[2]]

word_count_nested$plot[[11]]

```

With everything organized in a "tidy" way, let's try to plot for ALL of the albums!

First let's try with something we used in Part 1, facetting!

Before we start plotting we need to "unnest" the data from >>>

```{r fig.height=5, fig.width=8, fig.align='center'}

word_count_nested %>% 
  unnest(data) %>%                   # take data out from list
  ggplot(aes(x = word, y = count)) +
  geom_bar(stat = "identity") +
  facet_grid(.~album)

```

- can see that this won't work with facetting as we face the problem of there not being enough space for all the albums to fit in a single row.
- even then data will be hard to compare side-by-side.
- if facetting done the other way around with each album as a row stacked on top of another, still difficult to see the differences in the heights of the bar counts

What can we do?
One way to solve our problem is to code in a way that each plot for each album is printed out individually and then to arrange all those individual plots into one page.
This will solve the problem with facetting this group of plots won't be forcibly squished together into one gigantic plot.
- save all plots in as one list

```{r}

nested_plots <- word_count_nested$plot[1:4]

```

```{r}

glimpse(nested_plots)[[1]]
str(nested_plots, list.len = 2, max.level = 2)

```

```{r}

do.call(grid.arrange, c(nested_plots, ncol = 2))

```

```{r fig.height=5, fig.width=8, fig.align='center'}

# base R method with do.call() function:
do.call(grid.arrange, c(word_count_nested$plot, ncol = 3))

```



```{r fig.height=5, fig.width=8, fig.align='center'}

# works very easily with cowplot::plot_grid() function!
# call list of ggplots (per album) with plotlist = __ then specify # of columns/rows/etc...
library(cowplot)
plot_grid(plotlist = nested_plots, ncol = 3)

```

```{r}

# save plot of most frequent word (for each album) individually now! ####
# apply the function ggsave() so that it iteratively saves the plot for each album!
map2(paste0(word_count_nested$album, ".pdf"), word_count_nested$plot, ggsave)

```

In Part 3 we will look more closely at the different sentiments of the words in Thrice's lyrics!
-
-
-
-


