---
title: "Part2.r"
author: "Ryo Nakagawara"
date: "Mon Oct 09 17:39:31 2017"
---

Today in Part 2 we will look at the lyrical content of the band, Thrice. By dividing the lyrics of each song into a single word per row, we can take a much closer look at the components of the lyrical content on a song-level or at the album-level! 

Let's get started!

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

As always let's load the various packages we are going to be using!

```{r Packages}
# Packages:
library(tidyverse)  # for dplyr, tidyr, ggplot2
library(tidytext)   # for separating text into words with unnest_tokens() function
library(stringr)    # for string detection, extraction, manipulation, etc.

library(gplots)     # for a certain type of plots not in ggplot2
library(ggrepel)    # for making sure labels don't overlap
library(scales)     # for fixing and tweaking the scales on graphs
library(gridExtra)  # for arranging multiple plots into a single page

```

```{r include=FALSE}
library(lubridate)

df <- read.csv('thrice.df.csv', header=TRUE, stringsAsFactors = FALSE)

df <- df %>% 
       mutate(album = factor(album, levels = unique(album)),
              length = ms(length),
              lengthS = seconds(length))

```

First we have to load in the dataset that we finished tidying up in Part 1 (not shown here). 

Let's finally take a look at the actual LYRICS of Thrice. 

```{r lyrics string}

df %>% 
  select(lyrics) %>% 
  substr(4, 116)

```

Here we are looking at the first few lines of the first song in the first album. We can see that the lyrics are separated into lines by the <br> tag. Note that this is how the lines were separated from the source, [AZlyrics.com](https://www.azlyrics.com/t/thrice.html), and may not reflect how it is separated in the [album booklets](https://stitchesandgrooves.files.wordpress.com/2010/09/thrice-identity-crisis-insert.jpg) (as you can see, the first two lines shown above are actually one in the booklet).

For the purposes of this analysis and the slight discrepancy in the lines we will first break up the `lyrics` column into lines to get rid of the `<br>` tags and then split `lyrics` so that the data is in the one-word-per-row format. This proces is called **tokenizing** and we use the `unnest_tokens()` function in the `tidytext` package for restructuring text datasets!

Using `unnest_tokens()` we need to:
- Enter in the `output`: the column to be created from tokenizing.
- Enter in the `input`: the column that gets split or **tokenized**. 
- Enter in the `token`: the unit for tokenizing. Default is by "words".
- Enter in `to_lower`: to turn everything inside the specified column to lower case.   
- Other inputs and options can be found by looking at the help page: `?unnest_tokens`.

```{r unnest_tokens() wordToken}
library(stringr)
# use the stringr for str_split() function to split "lyrics" on the <br> tags!

wordToken <-  df %>%
  unnest_tokens(output = line, input = lyrics, token = str_split, pattern = ' <br>') %>%   
  unnest_tokens(output = word, input = line, to_lower = TRUE) 

glimpse(wordToken)

```

Now we have a dataset with all words separated by row.  

Now let's count the how many times each word appears throughout the lyrics!

```{r count word (wordToken)}

countWord <- wordToken %>% count(word, sort=TRUE)
countWord  %>% head(10)
# 'the' is most common....

```

Just from looking at this it is clear that this isn't very informative about the content of lyrics. Words such as 'I', 'you', 'we', 'very', 'the' aren't very useful for analyzing the meaningfulness of our dataset. These very common set of words are called "stop words". For example:

```{r stop words}

data("stop_words")

set.seed(1)
sample_stop <- stop_words %>% sample_n(10)

sample_stop

```

Using the built-in lexicons in the `tidytext` package ("onix", "SMART", and "snowball") we can create a new dataset where we filter out the "stop words"" from our `word` column in `wordToken`.  

This can be done by using `anti_join()` function which returns all rows from `x` (our original `wordtoken` dataset) where there are no matching values in `y` (`stop_words` dataset) on a variable with a common name across both datasets (`word`).

```{r wordToken2}

wordToken2 <- wordToken %>% 
  anti_join(stop_words) %>% # Take out rows of `word` in wordToken that appear in stop_words
  arrange(ID)               # Can also arrange by track_num, basically the same thing

countWord2 <- wordToken2 %>% count(word, sort = TRUE)

countWord2 %>% head(10)

```

With "stop words" being filtered out of our dataset, 'eyes', 'love', 'light', 'blood', and 'life' are the most common! We can make much more inferences about the lyrics from those compared to 'I', 'the', 'and', and 'to'!

Now that we have one dataset with "stop words" and one without, we can compare them using a visualization to really emphasize the importance of filtering out "stop words" from any text data:

```{r most common words comparison, fig.height=6, fig.width=8, fig.align='center'}

# graph of most common words (including stop words) 
one <- countWord %>% head(10) %>% 
  ggplot(aes(reorder(word, n), n)) + 
  geom_bar(stat = "identity", fill = "darkgreen", alpha = 0.75) +
  ggtitle("Comparison of 'Most Common Words'") +
  xlab("With 'stop words'") +
  scale_y_continuous(breaks = pretty_breaks(5)) +
  coord_flip() +
  theme_bw() +
  theme(panel.grid.major.x = element_line(size = 1.25),
        axis.text.x = element_text(size = 12, face = "bold"),
        plot.title = element_text(hjust = 0.5))

# graph of most common words (no stop words) 
two <- countWord2 %>% head(10) %>% 
  ggplot(aes(reorder(word, n), n)) + 
  geom_bar(stat = "identity", fill = "darkgreen", alpha = 0.75) +
  xlab("No 'stop words'") +
  scale_y_continuous(breaks = pretty_breaks()) +
  coord_flip() +
  theme_bw() +
  theme(panel.grid.major.x = element_line(size = 1.25),
        axis.text.x = element_text(size = 12, face = "bold"))

grid.arrange(one, two)

```

You can clearly see the difference between the datasets!

The fact that the scales for `n` are very different between the plot with "stop words" and the one without shows how individually meaningless "stop words" such as "the", "and", "to", and "a" can really disrupt our analyses. The plot without "stop words" gives us a much clearer idea of the most meaningful and common words in Thrice's lyrics!

Another way to see this obscure>> disruption in action is by visualizing our data in a different way, using word clouds!

```{r wordcloud - stop words, fig.height=5, fig.width=8, fig.align='center'}
library(wordcloud)
layout(matrix(c(1,2),1,2, byrow = TRUE))

wordcloud(words = countWord$word, freq = countWord$n, random.order = FALSE, max.words = 300, 
          colors = brewer.pal(8, "Dark2"), use.r.layout = TRUE)

wordcloud(countWord2$word, countWord2$n, random.order = FALSE, max.words = 300,
          colors = brewer.pal(8, "Dark2"), use.r.layout = TRUE)

```

With the word cloud visualization, we can really tell how the "stop words" in the left side obscures or "crowds out" all of the other more meaningful words due to the sheer amount of "the"s, "you"s, and "to"s that appear in the lyrics text.

# Data exploration

Now that we've spread out each word into it's own row, let's take a closer look at our new datasets!

```{r total song wordToken/2}

wordToken %>% select(title) %>% n_distinct()
wordToken2 %>% select(title) %>% n_distinct()

```

Both `wordToken` and `wordToken2` give the number of songs at 100... but wait! In Part 1 we checked that there were a total of 103 songs, in these "tokenized" datasets the instrumental songs were not included simply because as they do not have any words, so there is no row for those instrumentals to exist in these datasets!

```{r total songs df}

df %>% summarise(num_songs = n()) # 103 songs in total, as each row = 1 song

```

## Lyrics exploration

Let's create a new dataset called `WordsPerSong` that contains information inside columns of the song title and the word counts.

```{r Distr. Song by Word, fig.height=5, fig.width=8, fig.align='center'}

# WordsPerSong
WordsPerSong <- wordToken %>% 
  group_by(title) %>% 
  summarize(wordcounts = n()) %>%    # each row = 1 word
  arrange(desc(wordcounts))

WordsPerSong %>% 
  ggplot(aes(x = wordcounts)) + 
  geom_histogram(bins = 50, color = "white", fill = "darkgreen") +
  geom_vline(xintercept = median(WordsPerSong$wordcounts), 
             color = "red", linetype = "dashed", size = 1.25) +
  scale_y_continuous(breaks = pretty_breaks(), expand = c(0, 0), limits = c(0, 12)) +
  scale_x_continuous(breaks = pretty_breaks(10), expand = c(0, 0), limits = c(0, 410)) +
  xlab('Total # of Words') +
  ylab('# of Songs') +
  labs(title = 'Distribution of Songs by Number of Words \n (Dashed red line: median)') + 
  theme_bw() +
  theme(panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5))

```

The `wordToken` and `wordToken2` datasets unfortunately filters out the instrumentals all together, as the rows for the instrumentals are not populated by the `unnest_tokens()` function. Therefore, the median and mean values for word count will be slightly off in both the `wordToken` and `wordToken2` datasets.

Count the number of songs for each album, we did this in Part 1 with `df`, this time let's use the `wordToken2` dataset that we just created:

```{r Songs per album}

wordToken2 %>% 
  group_by(album) %>% 
  summarize(num_songs = n_distinct(title)) %>% 
  arrange(desc(num_songs))

```

Let's dig deeper, what about the number of words per song? We need to use `wordToken` instead of `df` or `wordToken2` as "stop_words"" should be included for the total word sum.

```{r Words per song}

wordToken %>% 
  select(title, album, word) %>% 
  group_by(title, album) %>% 
  summarize(num_word = n()) %>% 
  arrange(desc(num_word)) %>%
  head(10)

```

How about words per album? Let's also turn this info into a bar graph!

```{r Words per album}

wordToken %>% 
  select(album, word) %>% 
  group_by(album) %>% 
  summarize(num_word = n()) %>% 
  arrange(desc(num_word)) %>% 
  ggplot(aes(reorder(album, num_word), num_word, fill = num_word)) + 
  geom_bar(stat = "identity") + 
  scale_y_continuous(expand = c(0.01, 0)) +
  scale_fill_gradient(low = "#a1d99b", high = "#006d2c", guide = FALSE) +
  coord_flip() +
  theme_bw() +
  theme(axis.text.y = element_text(size = 8), axis.title.y = element_blank()) +
  ylab("Number of Words")

```

Individually, the Alchemy Index albums are the lowest as they each have only 6 songs each, if they were combined into their actual album sets (Volume 1: Fire & Water, Volume 2: Earth & Air), they would probably have more words than Identity Crisis.

## Some more exploration with `dplyr` verbs!

Use `filter()` to look at a specific album or specific song.
```{r}

wordToken %>% 
  filter(album == "Vheissu") %>% 
  summarize(num_word = n())

wordToken %>% 
  filter(title == "The Weight") %>% 
  summarize(num_word = n()) 
  
```

The Weight was the first Thrice song I listened to in my friend's dorm room back in college, so it has quite a sentimental value to me!

most common word in the lyrics for "The Weight"?

```{r}

wordToken2 %>% 
  filter(title == "The Weight") %>% 
  group_by(title) %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  head(5)

```

"won't", "leave", "love", "abandon", "burning" 
>>> sounds like a very

We can also combine `dplyr` with other functions... 
- use str_*() functions to find and dig deeper into specific words!
- Let's take a closer look at one of the most common words that we found, "light", and check the total number of times "light" appears in lyrics of song.

```{r}

wordToken2 %>% 
  str_count("light") %>% 
  sum()

```
We can see that across all the songs in Thrice's discography, the word "light" shows up 87 times!

Now we use `mutate()` to create a new column that gives us the number of times the word "light" appears for each song.

```{r}

wordToken2 %>% 
  select(title, album, word) %>%
  mutate(light = str_count(word, "light")) %>% 
  group_by(title, album) %>% 
  summarize(total_light = sum(light)) %>% 
  arrange(desc(total_light)) %>% 
  head(5)

```

```{r}
# proportion of "light" in all words
wordToken %>% 
  select(title, album, word) %>% 
  mutate(num_word = n()) %>% 
  mutate(prop_light = (sum(str_count("light")) / num_word))

```

-- most frequent word in each song

```{r}

wordToken2 %>% 
  group_by(title) %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  head(10)

```

- "I'll" in Black Honey >>> song about
- "image" and "invisible" in "Image of the Invisible" >>> given as shouted out during the chorus numerous
- times in the song... To consider: skewed toward phrases repeated in chorus!

Now let's take this to the album-level!


#################     NESTED MOST COMMON WORDS PER ALBUM

Create a "nested" dataset of most common words for EACH album
>>> basically, the "data" column will contain the specific most commond words for a certain album "row".
- nested on albums!

```{r}
# most frequent unigrams per album: ####

word_count_nested <- wordToken2 %>% 
  group_by(album, word) %>% 
  summarize(count = n()) %>% 
  top_n(5) %>% 
  arrange(album, desc(count)) %>% 
  nest() 

```

Let's take a look at the individual elements of our new "data" column!

```{r}

word_count_nested$data[[1]]

word_count_nested$data[[5]]

```

The most common word data for the first list (Album = Identity Crisis) and the fifth list (Album = AI: Fire)

The only problem with the top_n() function is that if there are ties than the total number will be bigger than `n`.

Now we use the data to create a plot for each album using the `map2()` function which allows us to iteratively create a plot from each specific `data` column from each `album` row and stores the plot information in its own column `plot`, just like we did in `data`.

```{r}

word_count_nested <- word_count_nested %>% 
  mutate(plot = map2(data, album, 
                     ~ggplot(data = .x, aes(fill = count)) +
           geom_bar(aes(reorder(word, count), count), 
                    stat = "identity", width = 0.65) +
           scale_y_continuous(breaks = pretty_breaks(10), limits = c(0, 22), expand = c(0, 0)) +
           scale_fill_gradient(low = "#a1d99b", high = "#006d2c", guide = FALSE) +
           ggtitle(.y) +
           labs(x = NULL, y = NULL) +
           coord_flip() +  
           theme_bw() +
           theme(axis.text.y = element_text(size = 7),
                 title = element_text(size = 10))
           ))

str(word_count_nested, list.len = 3, max.level = 2)

```

- the overall `word_count_nested` dataframe consists of 3 columns of album, data, and plot by 11 rows, one for each album.

- can now see that column "data" is series of lists that holds the top 10 or so words for each album (row)
>>> Ex. the first element is a data frame of 8 observations of 2 variables, specifically the 8 most common words in the first album as rows with word and count as the 2 column variables.
- "plot" column is a series of lists that holds the plot data for each album (row)

- by selecting the specific element within the list, we can extract the plot for a certain album

```{r fig.height=5, fig.width=8, fig.align='center'}

word_count_nested$plot[[2]]

word_count_nested$plot[[11]]

```

With everything organized in a "tidy" way, let's try to plot for ALL of the albums!

First let's try with something we used in Part 1, facetting!

Before we start plotting we need to "unnest" the data from >>>

```{r fig.height=5, fig.width=8, fig.align='center'}

word_count_nested %>% 
  unnest(data) %>%                   # take data out from list
  ggplot(aes(x = word, y = count)) +
  geom_bar(stat = "identity") +
  facet_grid(.~album)

```

- can see that this won't work with facetting as we face the problem of there not being enough space for all the albums to fit in a single row.
- even then data will be hard to compare side-by-side.
- if facetting done the other way around with each album as a row stacked on top of another, still difficult to see the differences in the heights of the bar counts

### What can we do?
One way to solve our problem is to code in a way that each plot for each album is printed out individually and then to arrange all those individual plots into one page.
This will solve the problem with facetting this group of plots won't be forcibly squished together into one gigantic plot.

One way ti to use 

- base R method with do.call() function to iterate the `grid.arrange()` function for each `plot` column in word_count_nested.

```{r fig.height=5, fig.width=8, fig.align='center'}

do.call(grid.arrange, c(word_count_nested$plot, ncol = 3))

```

Another way is to first...
- save all plots in as one list
- to get an output slightly different from the above, let's just make a subset of the 1st to 4th plots (Albums: Identity Crisis to The Artist In The Ambulance).

```{r}

nested_plots <- word_count_nested$plot[1:4]

```

```{r}

str(nested_plots, list.len = 2, max.level = 2)

```

-- A list with a length 4 for each of the 4 albums that we subsetted here.
-- Within each album list we have another set of lists for the respective album's `data` and `plot` elements!

- works very easily with cowplot::plot_grid() function!
- call list of ggplots (per album) with plotlist = __ then specify # of columns/rows/etc...

```{r fig.height=5, fig.width=8, fig.align='center'}

library(cowplot)
plot_grid(plotlist = nested_plots, ncol = 2)

```

- save plot of most frequent word (for each album) individually now! ####
- apply the function ggsave() so that it iteratively saves the plot for each album!

```{r}

map2(paste0(word_count_nested$album, ".pdf"), word_count_nested$plot, ggsave)

```

In Part 3 we will look more closely at the different sentiments of the words in Thrice's lyrics!
-
-
-
-


