---
title: "Part_3.r"
author: "Ryo Nakagawara"
date: "Fri Oct 13 21:31:45 2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

In Part 3 we get into the core element of our analysis, investigating the various sentiments behind Thrice's lyrics! 

Using the three lexicons included with the `tidytext` package, NCR, Bing, and AFINN we can categorize our tokenized lyrics dataset and then perform a variety of transformations and manipulations to create some visualizations and tables!

```{r packages }
library(stringr)

library(tidyverse)
library(tidytext)
library(lubridate)

library(ggrepel)
library(scales)
library(gplots)
library(gridExtra)

library(wordcloud)

```

```{r load from initial datasets, include = FALSE}
# load dataset ------------------------------------------------------------

df <- read.csv('thrice.df.csv', header = TRUE, stringsAsFactors = FALSE)

df <- df %>% 
  mutate(album = factor(album, levels = unique(album)),
         length = ms(length),
         lengthS = seconds(length))

wordToken <-  df %>%
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  unnest_tokens(word, line, to_lower = TRUE) 

countWord <- wordToken %>% count(word, sort=TRUE)

# Include stop_words ------------------------------------------------------

wordToken2 <- wordToken %>% 
  anti_join(stop_words) %>%                 
  arrange(ID)  # or track_num essentially same thing

countWord2 <- wordToken2 %>% count(word, sort=TRUE)

```


Let's get started!
Using the wordToken2 dataset that we created in __Part 2__ we will filter out any numbers in our data set and then use a `left_join()` function to:
- grab a specific sentiment lexicon with the `get_sentiments()` function
- "join" the sentiment lexicon to the **tokenized** dataset, specify `by = "word"`


```{r tidy_lyrics and emotions_lyrics_bing}
# Sentiment analysis ------------------------------------------------------

tidy_lyrics <- wordToken2 %>% 
  select(-writers, -length, -lengthS)

emotions_lyrics_bing <- tidy_lyrics %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  group_by(album) %>%   
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   # add in neutral
  ungroup()

emotions_lyrics_bing %>% 
  count(sentiment)

```

Unfortunately, the Bing lexicon counts most of the words in Thrice's lyrics as "neutral" as the 

Positive: 423, Negative: 912, Neutral: 5095

We'll come back to a more in-depth analysis of the nuances of each of the lexicons in the `tidytext` package a little bit later. 



^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

MOST COMMON POS/NEG + WORDCLOUD     BING

DISTRIBUTION OF EMOTION WORDS NRC

NET SENTIMENT RATIO      BING

>>> need for AFINN as well to comapre scores better...!

BIGRAMS AND TRIGRAMS(?)

NEGATION WORDS

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# Most common pos.neg words in Thrice lyrics! ####

To have words with negative sentiment to have negative numbers we can create an `if else` statement that assigns **-n** to a word with a negative sentiment and to all else assign the regular **n** count. We'll then order the words in these groups by the number of times they appear in the plot.

```{r Most common positive and negative words, fig.height=5, fig.width=8, fig.align='center'}

word_count <- emotions_lyrics_bing %>% 
  count(word, sentiment, sort = T) %>% 
  ungroup()

top_sentiments_bing <-  word_count %>% 
  filter(sentiment != 'neutral') %>% 
  group_by(sentiment) %>% 
  top_n(5, n) %>% 
  mutate(num = ifelse(sentiment == "negative", -n, n)) %>%  
  select(-n) %>% 
  mutate(word = reorder(word, num)) %>% 
  ungroup() 

ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c("black", "darkgreen")) +
  scale_y_continuous(limits = c(-40, 70), breaks = pretty_breaks(7)) + 
  labs(x = '', y = "Number of Occurrences",
       title = 'Lyrics Sentiment of Thrice',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))

```

Reminiscent of the most common words plot (without "stop words") from **Part 2** except this time we've differentiated between words with positive sentiment and negative sentiment! Another way to visualize this is using a wordcloud!

# Word Cloud: Most Common Positive and Negative Words in Thrice Lyrics ####

```{r wordcloud most common pos-neg, fig.height=5, fig.width=8, fig.align='center'}
library(wordcloud)

emotions_lyrics_bing %>% 
  filter(sentiment != 'neutral') %>% 
  count(word, sentiment, sort = T) %>% 
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("black", "darkgreen"), title.size = 1.5)

```
With the size of the word dictated by the size and central position in the cloud, this is another cool way to visualize the most common words. As seen before for positive words, __Love__ appears the most whereas for negative, __Fall__ appears most frequently.

##### with bing >>>> only Pos-Neg ####

OK, now that we've looked at the most common words for either positive or negative sentiment, what proportion of these sentiments are present in within the entire lyrical dataset? For this we can create 

After filtering out the words categorized as neutral, we calculate the frequency by first grouping them along `sentiment` (the column specifying all the different emotion terms) and then counting the rows for each of these groups. Finally, we can calculate the percentage by dividing by the sum of all the rows in the dataset.

```{r   OTHER EMOTIONS_LYRICS_BING, fig.height=7, fig.width=8, fig.align='center'}

pos_neg_bing <- tidy_lyrics %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  group_by(album, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>% 
  select(-n) %>% 
  ungroup() 

pos_neg_bing %>% 
  filter(sentiment != "neutral") %>% 
  ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = "Album", y = "Emotion Words Count (as %)") +
  scale_color_manual(values = c(positive = "darkgreen", negative = "black")) +
  ggtitle("Positive and Negative Word Count (as Percentage of Total)", subtitle = "Bing lexicon") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold"))

```

As we saw when we counted the words categorized for each sentiment at the beginning of the article, the **neutral** sentiment (not shown above) dominates, so both the positive and negative percentages move around a very low range. However, just looking at positive and negative sentiment groups may not be informative enough, now let's use the NRC lexicon to dig deeper!

# with nrc >>>>  Distribution of emotion words BOXPLOT: ####

The NRC lexicon not only categorizes words into "positive" and "negative" categories but also into eight different emotion terms, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

After filtering out the words categorized as positive/negative as we are only comparing between the emotion terms here, and do the same calculations as we did for the previous plot.

Let's use the colors from Set1 in the RColorBrewer palettes to differntiate our emotion terms in the plot. We'll use these assigned colors for some line charts later on too!

```{r get colors}

cols <- colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)

cols

```

"#E41A1C", "#377EB8", etc. are the color hex codes for the colors in "Set1". Now that we've extracted these color codes, let's assign them to the emotion categories and then specify those are the colors we want to use for our lines with `scale_color_manual()` function.

```{r assign colors, emotions_lyrics_NRC, fig.height=5, fig.width=8, fig.align='center'}

cols <- c("anger" = "#E41A1C", "sadness" = "#377EB8", "disgust" = "#4DAF4A", 
          "fear" = "#984EA3", "surprise" = "#FF7F00", "joy" = "#FFFF33", 
          "anticipation" = "#A65628", "trust" = "#F781BF")

emotions_lyrics_nrc <- tidy_lyrics %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(album, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>%   
  select(-n) %>% 
  ungroup() 

library(hrbrthemes)

emotions_lyrics_nrc %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  scale_fill_manual(values = cols) +
  ggtitle("Distribution of Emotion Terms", subtitle = "n = 11 (Albums)") +
  labs(x = "Emotion Term", y = "Percentage") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 11, face = "bold"),
        axis.text.y = element_text(size = 11, face = "bold"))

```

we can clearly see the box-plot distributions of the different emotion categories! As in any boxplot, the black bar inside the box signifies the median, the hinges represent the 25th and 75th percentiles, while the whiskers extend to the value of 1.5 * IQR of their respective hinges, and the black dots are the outliers. But suppose we want to see how the lyrics change sentiment over time? We can create a bump chart that plots the different sentiment groups for each album in order of release-date!

```{r nrc all sentiments, fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_nrc %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_brewer(palette = "Set1")
  
```

With EIGHT different emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust) in the NRC lexicon our nice graph looks quite cluttered and its hard to spot the trends, although it is quite clear that **The Alchemy Index: Earth** is the source of a lot of sudden increases/decreases in emotions.

To solve our current problem, let's split the emotions into groups to be able to see changes better!
These emotion terms won't perfectly group up but let's divide them into:
- anger, disgust, fear, and sadness 
- surprise, anticipation, joy, and trust 

```{r plot positive emotion,  fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_nrc %>% 
  filter(sentiment != "anger" & sentiment != "disgust" & 
         sentiment != "fear" & sentiment != "sadness") %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments: Positive Emotion Terms", subtitle = "Release-date order (2000-2016)") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_manual(values = cols, name = "Emotion Terms")

```

Anticipation seems to have dramatic dips in AI: Fire and AI: Earth compared to the preceding albums. Meanwhile Joy reaches the highest point for positive emotions at ~20% in AI: Earth while Trust seems to hover around 15-17% throughout Thrice's discography until the last few albums.
WHY AI: Earth?
And next, the negative emotions.

```{r plot negative emotions,  fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_nrc %>% 
  filter(sentiment != "anticipation" & sentiment != "joy" & 
         sentiment != "trust" & sentiment != "surprise") %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = "Album", y = "Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments: Negative Emotion Terms", subtitle = "Release-date order (2000-2016)") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_manual(values = cols, name = "Emotion Terms")

```

We can see that both Fear and Sadness have sudden spikes in AI:Fire and AI:Earth, these two albums and their relationship with their respective preceding and following albums warrant further inspection as it is clear there is a shift in emotional tone (for the different Alchemy Index albums the reason is quite obvious). Again, a caveat for this analysis is that the AI albums are only six songs each while the others average around 11 songs each.

# AI: Earth spike in positive from large decrease in "anger", small increase in "joy"

AI: Earth seems bereft in sentiments in general... besides joy and sadness
AI: Fire seems to have untrendly amount of FEAR: let's take a closer look!

```{r fear - nrc}

nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_lyrics %>% 
  filter(album == "The Alchemy Index Fire") %>% 
  inner_join(nrc_fear) %>% 
  count(word, sort = TRUE) %>% 
  head(5)

```

In AI: Fire, the word "fire" being tagged as Fear is what is mainly pushing the proportion of Fear up so high. The other words categorized, such as "fear", "buried", "die", and "gallows", definitely belong in this category as well.


```{r anger - nrc}

nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

tidy_lyrics %>% 
  filter(album == "The Alchemy Index Earth") %>% 
  inner_join(nrc_anger) %>% 
  count(word, sort = TRUE)

tidy_lyrics %>% 
  filter(str_detect(album, "The Alchemy")) %>% 
  inner_join(nrc_anger) %>% 
  count(album, word, sort = TRUE) %>% 
  group_by(album) %>% 
  summarize(n = n())

```

Words categorized as Anger only appear four times! In the other AI albums Anger words appear much more frequently (and the proportions would also drastically change as well)! A separate article looking at only the AI albums may be beneficial...

# Net sentiment ratio for each song ####

Similar to what we did when we looked at the proportion of positive, negative, and neutral words in our data, we can calculate a net sentiment ratio for each song.

```{r net sentiment ratio 1.0}

emotions_lyrics_bing %>% 
  group_by(title) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  # replace_na(replace = list(negative = 0, neutral = 0, positive = 0)) %>%  
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  "["(., 5:9, )

```

We can fill out any NAs in the sentiment columns into zeros by either using the `replace_na()` from the `tidyr` package OR in `spread()` use the option *fill =* and set it to 0!

Now let's create a new dataset that takes these sentiment ratios, group them by album, and then take the average for each album.

```{r net sentiment ratio album_means}

emotions_lyrics_bing_avg <- emotions_lyrics_bing %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>%   
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  ungroup() %>% 
  group_by(album) %>% 
  summarize(mean_album_ratio = mean(sentiment_ratio))

emotions_lyrics_bing_avg

```

Let's visualize this info!

# Lyrics sentiment ratio graph ####

```{r visualize net sentiment ratio mean_album, fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_bing_avg %>% 
  ggplot(aes(album, mean_album_ratio)) +      # reorder(album, sentiment_ratio), sentiment_ratio
  geom_col(fill = "darkgreen", alpha = 0.7) +  
  scale_fill_manual(guide = FALSE, values = c('#565b63', '#c40909')) +
  scale_y_percent(limits = c(-0.15, 0.10), breaks = pretty_breaks(7)) +    # from hrbrthemes
  theme_bw() +                     
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 0.95)) +
  ggtitle("Average Lyrics Sentiment Ratio (along album release)", 
          subtitle = "(Positive - Negative) / (Positive + Negative + Neutral), using the Bing lexicon") +
  labs(x = "Albums", y = "Sentiment Ratio (%)")

```

We've seen in previous plots that the sentiment of albums were more negative than positive and this plot confirms this as well. Why are Thrice categorized as so very negative? Let's look at how negative is defined by the three different lexicons.

Let's compare the lexicons themselves on how many "positive" and "negative" words they each categorize.

```{r}

get_sentiments("bing") %>% 
  count(sentiment)

get_sentiments("nrc") %>% 
  count(sentiment)

```

In the BING lexicon, there are far more negatively-categorized words than positive... more than twice as many in fact...

- In the Bing lexicon: there are 4782 words that can be categorized as "negative" along with 2006 "positive" words.
- In the NRC lexicon: there are 3324 words that can be categorized as "negative" along with 2312 "positive" words.

Now let's count how many words in the lyrics were categorized for each sentiment:

```{r}

emotions_lyrics_bing %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())

tidy_lyrics %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())
  
```

Both the NRC and Bing lexicons didn't have a category for most of the words in Thrice lyrics (neutral or NA) and there were more negative words than positive.

Let's see if this holds true with the AFINN lexicon, the only lexicon we haven't touched yet in the `tidytext` package! The AFINN lexicon gives a score from -5 (for negative sentiment) to +5 (positive sentiment). 

```{r AFINN scores}

emotions_lyrics_afinn <- tidy_lyrics %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(!grepl('[0-9]', word))

emotions_lyrics_afinn %>% 
  summarize(NAs= sum(is.na(score)))

emotions_lyrics_afinn %>% 
  select(score) %>% 
  mutate(sentiment = if_else(score > 0, "positive", "negative", "NA")) %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())
 
```


5220 of 6430 words in our lyrics dataset dont have AFINN score.... 使えない～～～～～

In line with the other lexicons, the majority of the words were categorized into NA (neutral)... then words were categorized as "negative" at a higher frequency compared to "positive". 

To create another visualization of , instead of taking the average for each album, we'll just take the sentiment score for each album and plot it along each song. This way we'll have more observations to compare the sentiment scoring ability of each lexicon.

```{r sentiments along song, fig.height=6, fig.width=8, fig.align='center'}

afinn_scores <- emotions_lyrics_afinn %>% 
  replace_na(replace = list(score = 0)) %>%
  group_by(index = title) %>% 
  summarize(sentiment = sum(score)) %>% 
  mutate(lexicon = "AFINN")

bing_nrc_scores <- bind_rows(
  tidy_lyrics %>% 
    inner_join(get_sentiments("bing")) %>% 
    mutate(lexicon = "Bing"),
  tidy_lyrics %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", "negative"))) %>% 
    mutate(lexicon = "NRC")) %>% 
  # from here we count the sentiments, spread on positive/negative, then create the score:
  count(lexicon, index = title, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

all_lexicons <- bind_rows(afinn_scores, bing_nrc_scores)
lexicon_cols <- c("AFINN" = "#E41A1C", "NRC" = "#377EB8", "Bing" = "#4DAF4A")

all_lexicons %>% 
  ggplot(aes(index, sentiment, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~lexicon, ncol = 1, scales = "free_y") +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle("Comparison of Sentiments", subtitle = " Along Song-Order") +
  labs(x = "Index of All Thrice Songs", y = "Sentiment Score") +
  theme_bw() +
  theme(axis.text.x = element_blank())

```

There are too many songs (103!) to show on the x-axis labels but I think you get the general idea!

The Bing lexicon sentiment along song order is generally negative, this lines up with what we saw in the average sentiment ratio plot where we had plotted along album order instead.

As we go through the songs from the first track on Identity Crisis, the eponymous "Identity Crisis" all the way down to the last track on last year's To Be Everywhere Is To Be Nowhere, "Salt And Shadow", the AFINN, Bing, and NRC lexicons seem to agree on the sentiments but differ in the magnitude of the expressed sentiment for a specific song.

The AFINN lexicon has the largest absolute magnitudes, however, remember that only 1210 out of 6430 total words in the lyrics were given a AFINN score. This means that the few words that do have scores exaggerate the magnitude of the sentiment scores relative to the other lexicons.

From **PART 2** we saw that in the song, "The Weight", "won't" was used in a positive sense in ___ with phrases such as "won't abandon" or "won't leave you"; could these negation words in front of positive words be messing up our sentiment calculations? We can figure this out by separating the lyrics text into bi-grams! Bi-grams are basically chunks of two words each in a text and we create a new dataset using the arguments in our trusty `unnest_tokens()` function!

## Ngrams! Splitting the text into slightly bigger word chunks!

To separate into bi-grams instead of single words-per-row we use the option *token =* and set it to "ngrams" and then set *n* to *2*. 

```{r}

biGrams <- df %>% 
  select(album, title, year, lyrics) %>% 
  mutate(lyrics = iconv(lyrics, to = 'latin1')) %>% 
  # convert to ASCII for better separation into ngrams (words with apostrophes)
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  # split lines on the <br> tags
  unnest_tokens(ngram, line, token = "ngrams", n = 2)

biGrams %>% head(5)

```

You can see that there is an overlap for each word. "satellite" appears as the second word in the first row and subsequently apepars as the first word in the second bigram row. We can then turn each bi-gram into two separate columns, "word1" and "word2" using the `separate()` function.

```{r}
biGrams_sep <- biGrams %>% 
  separate(ngram, c("word1", "word2"), sep = "[^-'\\w]")  

biGrams_sep %>% head(5)

```

At first I had a problem where `separate()` would ignore the apostrophes and therefore split "I'll" into "I" and "ll" which screwed up the process of splitting the bigrams into separate single word columns. To do this without any warning messages appearing, I had to convert all the character string vectors to ASCII format using the `iconv()` function.

For example: "I'll go" turned into `word1`: "I", `word2`: "ll", and ` `: "go" instead of `word1`: "I'll" and `word2`: "go".
After a lot of research and experimenting with different RegEx patterns I finally found a way for `separate()` to NOT delete the apostrophes and split the bigrams up properly. The RegEx used can be seen in the `sep = ` argument.

Now that we have a dataframe with the bigrams separated into their individual words. Let's try counting the most common bigrams:

```{r}

biGrams_sep %>% count(word1, word2, sort = TRUE) %>% head(10)

```

bigrams of "in   the", "of   the", "we   are" dominate. Not very meaningful!

Now let's filter our the stop_words from both `word1` and `word2` and then count up again for more meaningful bigrams!

```{r}

biGrams_sep_filtered <- biGrams_sep %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word)

biGrams_count <- biGrams_sep_filtered %>% 
  count(album, title, word1, word2, sort = TRUE)

biGrams_count %>% head(10)

```

"gotta" and "wake" appearing from the song titled "Wake Up" would be self-explanatory but what about in the song "A Living Dance Upon Dead Minds"?

```{r}

df %>% 
  filter(title == "A Living Dance Upon Dead Minds") %>% 
  unnest_tokens(lines, lyrics, token = str_split, pattern = " <br>" ) %>% 
  select(lines) %>% 
  head(10)

```

It appears that similar lines are repeated throughout the song but in this case we can't really call this a chorus... As noted in previous articles during the data gathering for lyrics it's necessary to make sure to look for sources that don't repeat the chorus or have the chorus as a separate section. As a side note, the disorganized syntax seen in the lyrics is from the poem this song was based on, "But If A Living Dance Upon Dead Minds" by E. E. Cummings.

Back to the topic, because we have separated each word in the bi-gram, we can now search for the variety of negation words in the column `word1` and check out what appears in `word2`.

# Negation words! ####

Let's create a character vector of common negation words such as "not", "no", "wouldn't", etc.

```{r negation vector}

negation_words <- c("not", "no", "never", "without", "won't", "don't", "wouldn't", "couldn't")

```

Then let's filter bi-grams that only have negation words in the `word1` column!

```{r negated bigrams}

negated_bigrams <- biGrams_sep %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word1, word2, score, sort = TRUE) %>% 
  ungroup()

negated_bigrams %>% head(10)

```

Using the AFINN lexicon's scoring scheme (negative sentiments as negative scores up to a magnitude of -5 and vice versa) we can look at how the words in `word2` are scored. "not  alone", "no  evil", "won't leave" are actually very positive but not counted as such. On the other hand, "don't  care" is counted as positive! As each negation word is scored as -1, this can overstate or understate the scores greatly!

Let's visualize this to get a clear idea of the miscalculation!

```{r, fig.height=6, fig.width=8, fig.align='center'}

negated_bigrams %>% 
  mutate(contribution = n * score,
         score = reorder(paste(word2, word1, sep = "__"), contribution),
         sentiment = if_else(contribution > 0, "positive", "negative")) %>% 
  group_by(word1) %>% 
  top_n(10, abs(contribution)) %>% 
  ggplot(aes(word2, contribution, fill = as.factor(sentiment))) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(guide = FALSE, values = c("black", "darkgreen")) +
  facet_wrap(~ word1, scales = "free") +
  coord_flip() +
  theme_bw() +
  theme(axis.text = element_text(size = 12),
        panel.grid.minor.x = element_line(size = 1.1),
        panel.grid.major.x = element_line(size = 1.1)) +
  labs(x = "Word Preceded by Negation Term", y = "Sentiment Score * Number of Occurences") 

```

We can see here that "no" is the most common negation word.

```{r}

biGrams_sep %>% 
  filter(word1 %in% "no") %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word2, score, sort = TRUE) %>% 
  ungroup() %>% 
  ggplot(aes(reorder(word2, score), score)) +
  geom_col(show.legend = FALSE) +
  coord_flip()

```



how about with the BING lexicon we've been using to sum up Positive/Negative?

```{r}

negation_words <- c("not", "no", "never", "without", "won't", "don't", "wouldn't", "couldn't")

negated_bigrams <- biGrams_sep %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) %>% 
  count(word1, word2, sentiment, sort = TRUE) %>% 
  ungroup()

negated_bigrams %>% head(10)

```



Solution:

Sum up the scores for both word1 and word2 as the overall sentiment score and then visualize?


```{r}

biGrams_sep %>% 
  select(word1, word2) %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) #%>% 
  #mutate(negate_sentiment = inner_join(get_sentiments("bing"), by = c(word1 = "word")))
  





```





Keep in mind that the lexicons in the `tidytext` package is not the be all and end all for text/sentiment analysis. One can even create their own lexicons through crowd-sourcing (such as Amazon MechanicalTurk, which is how some of the lexicons here were created) and from utilizing word lists accrued by your own company throughout the years dealing with customer/employee feedback, the sources are limitless!






