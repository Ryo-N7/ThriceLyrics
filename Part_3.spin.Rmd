---
title: "Part_3.r"
author: "Ryo Nakagawara"
date: "Fri Oct 13 21:31:45 2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

In Part 3 we get into the core element of our analysis, investigating the various sentiments behind Thrice's lyrics! 

Using the three lexicons included with the `tidytext` package, NCR, Bing, and AFINN we can categorize our tokenized lyrics dataset and then perform a variety of transformations and manipulations to create some visualizations and tables!

```{r packages }
library(stringr)

library(tidyverse)
library(tidytext)
library(lubridate)

library(ggrepel)
library(scales)
library(gplots)
library(gridExtra)

library(wordcloud)

```

```{r load from initial datasets, include = FALSE}
# load dataset ------------------------------------------------------------

df <- read.csv('thrice.df.csv', header = TRUE, stringsAsFactors = FALSE)

df <- df %>% 
  mutate(album = factor(album, levels = unique(album)),
         length = ms(length),
         lengthS = seconds(length))

wordToken <-  df %>%
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  unnest_tokens(word, line, to_lower = TRUE) 

countWord <- wordToken %>% count(word, sort=TRUE)

# Include stop_words ------------------------------------------------------

wordToken2 <- wordToken %>% 
  anti_join(stop_words) %>%                 
  arrange(ID)  # or track_num essentially same thing

countWord2 <- wordToken2 %>% count(word, sort=TRUE)

```


Let's get started!
Using the wordToken2 dataset that we created in __Part 2__ we will filter out any numbers in our data set and then use a `left_join()` function to:
- grab a specific sentiment lexicon with the `get_sentiments()` function
- "join" the sentiment lexicon to the **tokenized** dataset, specify `by = "word"`


```{r tidy_lyrics and emotions_lyrics_bing}
# Sentiment analysis ------------------------------------------------------

tidy_lyrics <- wordToken2 %>% 
  select(-writers, -length, -lengthS)

emotions_lyrics_bing <- tidy_lyrics %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  group_by(album) %>%   
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   # add in neutral
  ungroup()

emotions_lyrics_bing %>% 
  count(sentiment)

```

Unfortunately, the Bing lexicon counts most of the words in Thrice's lyrics as "neutral" as the 

Positive: 423, Negative: 912, Neutral: 5095

We'll come back to a more in-depth analysis of the nuances of each of the lexicons in the `tidytext` package a little bit later. 



^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

MOST COMMON POS/NEG + WORDCLOUD     BING

DISTRIBUTION OF EMOTION WORDS NRC

NET SENTIMENT RATIO      BING

>>> need for AFINN as well to comapre scores better...!

BIGRAMS AND TRIGRAMS(?)

NEGATION WORDS

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# Most common pos.neg words in Thrice lyrics! ####

To have words with negative sentiment to have negative numbers we can create an `if else` statement that assigns **-n** to a word with a negative sentiment and to all else assign the regular **n** count. We'll then order the words in these groups by the number of times they appear in the plot.

```{r Most common positive and negative words, fig.height=5, fig.width=8, fig.align='center'}

word_count <- emotions_lyrics_bing %>% 
  count(word, sentiment, sort = T) %>% 
  ungroup()

top_sentiments_bing <-  word_count %>% 
  filter(sentiment != 'neutral') %>% 
  group_by(sentiment) %>% 
  top_n(5, n) %>% 
  mutate(num = ifelse(sentiment == "negative", -n, n)) %>%  
  select(-n) %>% 
  mutate(word = reorder(word, num)) %>% 
  ungroup() 

ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c("black", "darkgreen")) +
  scale_y_continuous(limits = c(-40, 70), breaks = pretty_breaks(7)) + 
  labs(x = '', y = "Number of Occurrences",
       title = 'Lyrics Sentiment of Thrice',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))

```

Reminiscent of the most common words plot (without "stop words") from **Part 2** except this time we've differentiated between words with positive sentiment and negative sentiment! Another way to visualize this is using a wordcloud!

# Word Cloud: Most Common Positive and Negative Words in Thrice Lyrics ####

```{r wordcloud most common pos-neg, fig.height=5, fig.width=8, fig.align='center'}
library(wordcloud)

emotions_lyrics_bing %>% 
  filter(sentiment != 'neutral') %>% 
  count(word, sentiment, sort = T) %>% 
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("black", "darkgreen"), title.size = 1.5)

```
With the size of the word dictated by the size and central position in the cloud, this is another cool way to visualize the most common words. As seen before for positive words, __Love__ appears the most whereas for negative, __Fall__ appears most frequently.

##### with bing >>>> only Pos-Neg ####

OK, now that we've looked at the most common words for either positive or negative sentiment, what proportion of these sentiments are present in within the entire lyrical dataset? For this we can create 

After filtering out the words categorized as neutral, we calculate the frequency by first grouping them along `sentiment` (the column specifying all the different emotion terms) and then counting the rows for each of these groups. Finally, we can calculate the percentage by dividing by the sum of all the rows in the dataset.

```{r   OTHER EMOTIONS_LYRICS_BING, fig.height=7, fig.width=8, fig.align='center'}

pos_neg_bing <- tidy_lyrics %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  group_by(album, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>% 
  select(-n) %>% 
  ungroup() 

pos_neg_bing %>% 
  filter(sentiment != "neutral") %>% 
  ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = "Album", y = "Emotion Words Count (as %)") +
  scale_color_manual(values = c(positive = "darkgreen", negative = "black")) +
  ggtitle("Positive and Negative Word Count (as Percentage of Total)", subtitle = "Bing lexicon") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold"))

```

As we saw when we counted the words categorized for each sentiment at the beginning of the article, the **neutral** sentiment (not shown above) dominates, so both the positive and negative percentages move around a very low range. However, just looking at positive and negative sentiment groups may not be informative enough, now let's use the NRC lexicon to dig deeper!

# with nrc >>>>  Distribution of emotion words BOXPLOT: ####

The NRC lexicon not only categorizes words into "positive" and "negative" categories but also into eight different emotion terms, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

After filtering out the words categorized as positive/negative as we are only comparing between the emotion terms here, and do the same calculations as we did for the previous plot.

Let's use the colors from Set1 in the RColorBrewer palettes to differntiate our emotion terms in the plot. We'll use these assigned colors for some line charts later on too!

```{r get colors}

cols <- colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)

cols

```

"#E41A1C", "#377EB8", etc. are the color hex codes for the colors in "Set1". Now that we've extracted these color codes, let's assign them to the emotion categories and then specify those are the colors we want to use for our lines with `scale_color_manual()` function.

```{r assign colors, emotions_lyrics_NRC, fig.height=5, fig.width=8, fig.align='center'}

cols <- c("anger" = "#E41A1C", "sadness" = "#377EB8", "disgust" = "#4DAF4A", 
          "fear" = "#984EA3", "surprise" = "#FF7F00", "joy" = "#FFFF33", 
          "anticipation" = "#A65628", "trust" = "#F781BF")

emotions_lyrics_nrc <- tidy_lyrics %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(album, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>%   
  select(-n) %>% 
  ungroup() 

library(hrbrthemes)

emotions_lyrics_nrc %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  scale_fill_manual(values = cols) +
  ggtitle("Distribution of Emotion Terms", subtitle = "n = 11 (Albums)") +
  labs(x = "Emotion Term", y = "Percentage") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 11, face = "bold"),
        axis.text.y = element_text(size = 11, face = "bold"))

```

we can clearly see the box-plot distributions of the different emotion categories! As in any boxplot, the black bar inside the box signifies the median, the hinges represent the 25th and 75th percentiles, while the whiskers extend to the value of 1.5 * IQR of their respective hinges, and the black dots are the outliers. But suppose we want to see how the lyrics change sentiment over time? We can create a bump chart that plots the different sentiment groups for each album in order of release-date!

```{r nrc all sentiments, fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_nrc %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_brewer(palette = "Set1")
  
```

With EIGHT different emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust) in the NRC lexicon our nice graph looks quite cluttered and its hard to spot the trends, although it is quite clear that **The Alchemy Index: Earth** is the source of a lot of sudden increases/decreases in emotions.

To solve our current problem, let's split the emotions into groups to be able to see changes better!
These emotion terms won't perfectly group up but let's divide them into:
- anger, disgust, fear, and sadness 
- surprise, anticipation, joy, and trust 

```{r plot positive emotion,  fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_nrc %>% 
  filter(sentiment != "anger" & sentiment != "disgust" & 
         sentiment != "fear" & sentiment != "sadness") %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments: Positive Emotion Terms", subtitle = "Release-date order (2000-2016)") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_manual(values = cols, name = "Emotion Terms")

```

And now for the more negative emotions...
Anticipation seems to have dramatic dips in AI: Fire and AI: Earth
WHY AI: Earth?

```{r plot negative emotions,  fig.height=6, fig.width=8, fig.align='center'}

emotions_lyrics_nrc %>% 
  filter(sentiment != "anticipation" & sentiment != "joy" & 
         sentiment != "trust" & sentiment != "surprise") %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments: Negative Emotion Terms", subtitle = "Release-date order (2000-2016)") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_manual(values = cols, name = "Emotion Terms")

```

# ALthough "fear" has jumped up in AI:Fire outlier by more than 10%!
# also sudden dip in "anger" in AI: Earth as seen in previous plot!

# AI: Earth spike in positive from large decrease in "anger", small increase in "joy"
# decrease in anticipation as well....
AI: Earth seems bereft in sentiments in general... besides joy and sadness
AI: Fire seems to have untrendly amount of FEAR: let's take a closer look!

```{r fear - nrc}

nrcfear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_lyrics %>% 
  filter(album == "The Alchemy Index Fire") %>% 
  inner_join(nrcfear) %>% 
  count(word, sort = TRUE)

```

# FIRE being tagged as "fear" is what's mainly pushing up the trend
# fear, buried, die, gallows etc. are also in which is more in line with "fear" group

```{r anger - nrc}

nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

tidy_lyrics %>% 
  filter(album == "The Alchemy Index Earth") %>% 
  inner_join(nrc_anger) %>% 
  count(word, sort = TRUE)

# appears only in 4 words!
# in other AI albums appear much more frequently!

tidy_lyrics %>% 
  filter(str_detect(album, "The Alchemy")) %>% 
  inner_join(nrc_anger) %>% 
  count(album, title, word, sort = TRUE)

```



# Net sentiment ratio for each song 


```{r net sentiment ratio 1.0}

emotions_lyrics_bing %>% 
  group_by(title) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n) %>% 
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  "["(.,5:9,)

```

NAs in some sentiment columns >>> convert to ZEROs...
use `replace_na()` from the `tidyr` package!
OR 
in `spread()` use the option fill = and set it to 0!

```{r net sentiment ratio 2.0}

emotions_lyrics_bing %>% 
  group_by(title) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  # replace_na(replace = list(negative = 0, neutral = 0, positive = 0)) %>%  
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  "["(., 5:9, )

```

BY ALBUM

```{r net sentiment ratio album_means}

emotions_lyrics_bing %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>%   
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  ungroup() %>% 
  group_by(album) %>% 
  summarize(mean_album_ratio = mean(sentiment_ratio))

```


Now let's visualize this info!

# Lyrics sentiment ratio graph by ALBUM ####

with NEUTRAL
using BING lexicon.
geom_col() instead of geom_bar(stat = "identity") >>> basically the same thing but much better to not have to type `stat = "identity"` every single time!

```{r visualize net sentiment ratio mean_album}

emotions_lyrics_bing %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>%   
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  ungroup() %>% 
  group_by(album) %>% 
  summarize(mean_album_ratio = mean(sentiment_ratio)) %>% 
  ggplot(aes(album, mean_album_ratio)) +      # reorder(album, sentiment_ratio), sentiment_ratio
  geom_col(fill = "darkgreen", alpha = 0.7) +  
  # aes(fill = sentiment_ratio > 0)   irrelevant as ALL negative...
  scale_fill_manual(guide = FALSE, values = c('#565b63', '#c40909')) +
  scale_y_percent(limits = c(-0.15, 0.10), breaks = pretty_breaks(7)) +    # from hrbrthemes
  theme_bw() +                     
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 0.95)) +
  ggtitle("Lyrics Sentiment Ratio (along album release)", 
          subtitle = "(Positive-Negative) / (Positive + Negative + Neutral), using the Bing lexicon") +
  labs(x = "Albums", y = "Sentiment Ratio (%)")

```


why negative? look at HOW negative is defined.
negative talk but with positive action with negative undertones???? 

let's see if this holds true with the AFINN lexicon, the only lexicon we haven't touched yet in the `tidytext` package!

```{r AFINN scores}

emotions_lyrics_afinn <- tidy_lyrics %>% 
  left_join(get_sentiments("afinn"), by = "word") 

emotions_lyrics_afinn %>% 
  summarize(NAs= sum(is.na(score)))
 
```

5226 of 6436 dont have AFINN score.... 使えない～～～～～


```{r sentiments along song}

afinn_scores <- emotions_lyrics_afinn %>% 
  replace_na(replace = list(score = 0)) %>%
  group_by(index = title) %>% 
  summarize(sentiment = sum(score)) %>% 
  mutate(lexicon = "AFINN")

bing_nrc_scores <- bind_rows(
  tidy_lyrics %>% 
    inner_join(get_sentiments("bing")) %>% 
    mutate(lexicon = "Bing"),
  tidy_lyrics %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", "negative"))) %>% 
    mutate(lexicon = "NRC")) %>% 
  # from here we count the sentiments, spread on positive/negative, then create the score:
  count(lexicon, index = title, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

all_lexicons <- bind_rows(afinn_scores, bing_nrc_scores)
lexicon_cols <- c("AFINN" = "#E41A1C", "NRC" = "#377EB8", "Bing" = "#4DAF4A")

all_lexicons %>% 
  ggplot(aes(index, sentiment, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~lexicon, ncol = 1, scales = "free_y") +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle("Comparison of Sentiments", subtitle = " Along Song-Order") +
  labs(x = "Index of All Thrice Songs", y = "Sentiment Score") +
  theme_bw() +
  theme(axis.text.x = element_blank())

```

There are too many songs to be able to label them on the x-axis properly but I think you can get the general idea!

The Bing lexicon sentiment along song order is generally negative. This confirms what we saw in the lyrics sentiment ratio (with the Bing lexicon) where we had visualized along album order instead.

As we go through the songs from the first track on Identity Crisis, the eponymous "Identity Crisis" all the way down to the last track on last year's To Be Everywhere Is To Be Nowhere, "Salt And Shadow", the AFINN, Bing, and NRC lexicons seem to agree on the sentiments but differ in the magnitude of the expressed sentiment for a specific song.

Let's compare the lexicons on how many "positive" and "negative" words they each have.

```{r}

get_sentiments("bing") %>% 
  count(sentiment)

get_sentiments("nrc") %>% 
  count(sentiment)

```

in the BING lexicon, there are far more negatively-categorized words than positive... more than twice as many in fact...

In the Bing lexicon: there are 4782 words that can be categorized as "negative" along with 2006 "positive" words.

In the NRC lexicon: there are 3324 words that can be categorized as "negative" along with 2312 "positive" words.

```{r}

emotions_lyrics_afinn %>% 
  select(score) %>% 
  mutate(sentiment = if_else(score > 0, "positive", "negative", "NA")) %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())

emotions_lyrics_bing %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())

tidy_lyrics %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())
  
```

In all cases neutral or NA were the majority of the words...
In all the lexicons, words were categorized much more as "negative" compared to "positive". However, from PART 2 we saw that in the song, "The Weight", "won't" was used in a positive sense in ___ with phrases such as "won't abandon" or "won't leave you"; could these negation words in front of positive words be messing up our sentiment calculations? We can figure this out by separating the lyrics text into bi-grams!

Bi-grams are basically chunks of two words each in a text and we create a new dataset using the arguments in our trusty `unnest_tokens()` function!

appearance of Negation Words in THrice's lyrics!







>>> investigate bi-grams and tri-grams

## Ngrams! Splitting the text into slightly bigger word chunks!

At first I had a problem where `separate()` would ignore the apostrophes and therefore split "I'll" into "I" and "ll" which screwed up the process of splitting the bigrams into separate columns of the two individual words. 

To do this without any warning messages appearing, I had to convert all the character string vectors to ASCII format using the `iconv()` function.

For example: "I'll go" turned into `word1`: "I", `word2`: "ll", and ` `: "go" instead of `word1`: "I'll" and `word2`: "go".
After a lot of research and experimenting with different RegEx patterns I finally found a way for `separate()` to NOT delete the apostrophes and split the bigrams up properly below:

```{r}

biGrams <- df %>% 
  select(album, title, year, lyrics) %>% 
  mutate(lyrics = iconv(lyrics, to = 'latin1')) %>% 
  # convert to ASCII for better separation into ngrams
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  # split lines on the <br> tags
  unnest_tokens(ngram, line, token = "ngrams", n = 2)

biGrams_sep <- biGrams %>% 
  separate(ngram, c("word1", "word2"), sep = "[^-'\\w]")  

biGrams_sep

```

Now we have a dataframe with the bigrams separated into their individual words. Let's try counting the most common bigrams:

```{r}

biGrams_sep %>% count(word1, word2, sort = TRUE) %>% head(10)

```

bigrams of "in   the", "of   the", "we   are" dominate. Not very meaningful!

Now let's filter our the stop_words from both `word1` and `word2` and then count up again for more meaningful bigrams!

```{r}

biGrams_sep_filtered <- biGrams_sep %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word)

biGrams_count <- biGrams_sep_filtered %>% 
  count(word1, word2, sort = TRUE)

biGrams_count %>% head(10)

```








#### Negation words! How much does it screw up the sentiment scores?


```{r}

negation_words <- c("not", "no", "never", "without", "won't", "don't", "wouldn't", "couldn't")

negated_bigrams <- biGrams_sep %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word1, word2, score, sort = TRUE) %>% 
  ungroup()

negated_bigrams %>% head(10)

```

"not  alone", "no  evil", "won't leave"  are actually very positive but not counted as such.
On the other hand, "don't  care" is counted as positive!

Let's visualize this to get a clear idea of the miscalculation!

```{r}

negated_bigrams %>% 
  mutate(contribution = n * score,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>% 
  group_by(word1) %>% 
  top_n(10, abs(contribution)) %>% 
  ggplot(aes(word2, contribution, fill = n * score > 0)) +
  facet_wrap(~ word1, scales = "free") +
  geom_col(show.legend = FALSE) +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Word preceded by negation term") +
  ylab("sentiment score * number of occurences") + 
  coord_flip() +
  theme_bw()

```


how about with the BING lexicon we've been using to sum up Positive/Negative?

```{r}

negation_words <- c("not", "no", "never", "without", "won't", "don't", "wouldn't", "couldn't")

negated_bigrams <- biGrams_sep %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) %>% 
  count(word1, word2, sentiment, sort = TRUE) %>% 
  ungroup()

negated_bigrams %>% head(10)

```



Solution:

Sum up the scores for both word1 and word2 as the overall sentiment score and then visualize?


```{r}

biGrams_sep %>% 
  select(word1, word2) %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) #%>% 
  #mutate(negate_sentiment = inner_join(get_sentiments("bing"), by = c(word1 = "word")))
  





```





Keep in mind that the lexicons in the `tidytext` package is not the be all and end all for text/sentiment analysis. One can even create their own lexicons through crowd-sourcing (such as Amazon MechanicalTurk, which is how some of the lexicons here were created) and from utilizing word lists accrued by your own company throughout the years dealing with customer/employee feedback, the sources are limitless!






