---
title: "Part_3.r"
author: "Ryo Nakagawara"
date: "Fri Oct 13 21:31:45 2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r include = FALSE }
library(stringr)

library(tidyverse)
library(tidytext)
library(lubridate)

library(ggrepel)
library(scales)
library(gplots)
library(gridExtra)

library(wordcloud)

# load dataset ------------------------------------------------------------

df <- read.csv('thrice.df.csv', header = TRUE, stringsAsFactors = FALSE)

df <- df %>% 
  mutate(album = factor(album, levels = unique(album)),
         length = ms(length),
         lengthS = seconds(length))

wordToken <-  df %>%
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  unnest_tokens(word, line, to_lower = TRUE) 

countWord <- wordToken %>% count(word, sort=TRUE)

# Include stop_words ------------------------------------------------------

wordToken2 <- wordToken %>% 
  anti_join(stop_words) %>%                 
  arrange(ID)  # or track_num essentially same thing

countWord2 <- wordToken2 %>% count(word, sort=TRUE)

```


Let's get started!
Using the wordToken2 dataset that we created in __Part 2__ we will filter out any numbers in our data set and then use a `left_join()` function to 


```{r tidy_lyrics and emotions_lyrics_bing}
# Sentiment analysis ------------------------------------------------------

tidy_lyrics <- wordToken2 %>% 
  select(-writers, -length, -lengthS)

emotions_lyrics_bing <- tidy_lyrics %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  group_by(album) %>%   
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   # add in neutral
  ungroup()

```

```{r count all sentiments BING}
emotions_lyrics_bing %>% 
  count(sentiment)

```

Positive: 423, Negative: 912, Neutral: 5095

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

MOST COMMON POS/NEG + WORDCLOUD     BING

DISTRIBUTION OF EMOTION WORDS NRC

NET SENTIMENT RATIO      BING

>>> need for AFINN as well to comapre scores better...!

BIGRAMS AND TRIGRAMS

NEGATION WORDS

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# Most common pos.neg words in THrice lyrics! ####

```{r Most common positive and negative words}

word_count <- emotions_lyrics_bing %>% 
  count(word, sentiment, sort = T) %>% 
  ungroup()

top_sentiments_bing <-  word_count %>% 
  filter(sentiment != 'neutral') %>% 
  group_by(sentiment) %>% 
  top_n(5, wt = n) %>% 
  mutate(num = ifelse(sentiment == "negative", -n, n)) %>%  
  # count of negative words as negative #s!
  mutate(word = reorder(word, num)) %>% 
  select(word, sentiment, num)

# plot the occurences of the most common pos-neg words!

ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c("black", "darkgreen")) +
  scale_y_continuous(limits = c(-40, 70), breaks = pretty_breaks(12)) + 
  # c(-40, -25, -10, 0, 10, 25, 40, 55, 70)
  labs(y = "Number of Occurrences",
       x = '',
       title = 'Lyrics Sentiment of Thrice',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))

```

# separate between negative and positive words
# order by number of occurences
another way to visualize this is using a wordcloud!

# Word cloud: Most common Pos-Neg words in Thrice lyrics ####

```{r wordcloud most common pos-neg}
library(wordcloud)

emotions_lyrics_bing %>% 
  filter(sentiment != 'neutral') %>% 
  count(word, sentiment, sort = T) %>% 
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("black", "darkgreen"), title.size = 1.5)

```

# LOVE appears most, free, faith, perfect, grace...
# FALL appears most, dead, burn, fear, sick...

##### with bing >>>> only Pos-Neg-Neut ####

```{r   OTHER EMOTIONS_LYRICS_BING}

pos_neg_bing <- tidy_lyrics %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round(freq / sum(freq)*100)) %>% 
  select(-freq) %>% 
  ungroup() 

pos_neg_bing %>% 
  filter(sentiment != "neutral") %>% 
  ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  xlab("Album") + ylab("Emotion Words Count (as %)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(values = c(positive = "darkgreen", negative = "black")) +
  ggtitle("Positive/Negative Word Count as Percentage, along album release")

```

neutral dominate the categorization so both positive and negative in very low ranges.


# with nrc >>>>  Distribution of emotion words BOXPLOT: ####

fitler out the words categorized as positive/negative... only comparing between the emotion terms.

```{r emotions_lyrics_NRC }

emotions_lyrics_nrc <- tidy_lyrics %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = freq / sum(freq)) %>%   # round()   *100
  select(-freq) %>% 
  ungroup() 

library(hrbrthemes)

emotions_lyrics_nrc %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  theme_bw() +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  ggtitle("Distribution of Emotion words", subtitle = "n = all 11 Albums") +
  labs(x = "Emotion Term", y = "Percentage")

```

we can clearly see the box-plot distributions of the different emotion categories!
# black bar = mean, black dots = outliers

# NO pos or neg >>> anger, anticipation, disgust, fear, joy, sadness, surprise, trust

```{r nrc all sentiments}

emotions_lyrics_nrc %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_color_brewer(palette = "Set1")
  
```

# Rather messy. no noticeable trends to be found...! 
# ALthough "fear" has started to creep up after AI:Fire outlier.
# also sudden dip in "anger" in AI: Earth as seen in previous plot!
# AI: Earth in general causes a lot of sudden increase/decrease in emotions...

With EIGHT different emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust) our nice graph looks quite cluttered...

To solve this problem, let's split the emotions into groups to be able to see changes better!

set sentiment as factors?
>>> easier to assign to manual color scale?

NEG
anger, disgust, fear, sadness      
>>> colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)[1:4]

POS
surprise, anticipation, joy, trust 
>>> colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)[5:8]

Let's use the colors from Set1 in the RColorBrewer palettes for our emotions.

```{r get colors}
cols <- colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)

cols

```

Now that we've extracted the colors from Set1 let's assign them to the emotion categories.

```{r assign colors and plot positive emotions}

cols <- c("anger" = "#E41A1C", "sadness" = "#377EB8", "disgust" = "#4DAF4A", 
          "fear" = "#984EA3", "surprise" = "#FF7F00", "joy" = "#FFFF33", 
          "anticipation" = "#A65628", "trust" = "#F781BF")

emotions_lyrics_nrc %>% 
  filter(sentiment != "anger" & sentiment != "disgust" & 
         sentiment != "fear" & sentiment != "sadness") %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_color_manual(values = cols)

```

And now for the more negative emotions...

```{r plot negative emotions}

emotions_lyrics_nrc %>% 
  filter(sentiment != "positive" & sentiment != "negative" &
         sentiment != "anticipation" & sentiment != "joy" & 
         sentiment != "trust" & sentiment != "surprise") %>% 
  ggplot(aes(album, percent, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_color_manual(values = cols)

```


# AI: Earth spike in positive from large decrease in "anger", small increase in "joy"
# decrease in anticipation as well....
AI: Earth seems bereft in sentiments in general... besides joy and sadness
AI: Fire seems to have untrendly amount of FEAR: let's take a closer look!

```{r fear - nrc}

nrcfear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_lyrics %>% 
  filter(album == "The Alchemy Index Fire") %>% 
  inner_join(nrcfear) %>% 
  count(word, sort = TRUE)

```

# FIRE being tagged as "fear" is what's mainly pushing up the trend
# fear, buried, die, gallows etc. are also in which is more in line with "fear" group

```{r anger - nrc}

nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

tidy_lyrics %>% 
  filter(album == "The Alchemy Index Earth") %>% 
  inner_join(nrc_anger) %>% 
  count(word, sort = TRUE)

# appears only in 4 words!
# in other AI albums appear much more frequently!

tidy_lyrics %>% 
  filter(str_detect(album, "The Alchemy")) %>% 
  inner_join(nrc_anger) %>% 
  count(album, title, word, sort = TRUE)

```

group_by(album) %>% summarize() >>>>>???


# Net sentiment ratio for each song 


```{r net sentiment ratio 1.0}

emotions_lyrics_bing %>% 
  group_by(title) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n) %>% 
  mutate(sentiment_ratio = (positive - negative) / (positive + negative)) %>% 
  "["(.,5:9,)

```

NAs in some sentiment columns >>> convert to ZEROs...
use `replace_na()` from the `tidyr` package!
OR 
in `spread()` use the option fill = and set it to 0!

```{r net sentiment ratio 2.0}

emotions_lyrics_bing %>% 
  group_by(title) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  # replace_na(replace = list(negative = 0, neutral = 0, positive = 0)) %>%  
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  "["(., 5:9, )

```

BY ALBUM

```{r net sentiment ratio album_means}

emotions_lyrics_bing %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>%   
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  ungroup() %>% 
  group_by(album) %>% 
  summarize(mean_album_ratio = mean(sentiment_ratio))

```


Now let's visualize this info!
# Lyrics sentiment ratio graph by ALBUM ####
with NEUTRAL
using BING lexicon.
geom_col() instead of geom_bar(stat = "identity") >>> basically the same thing but much better to not have to type `stat = "identity"` every single time!

```{r visualize net sentiment ratio mean_album}

emotions_lyrics_bing %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>%   
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  ungroup() %>% 
  group_by(album) %>% 
  summarize(mean_album_ratio = mean(sentiment_ratio)) %>% 
  ggplot(aes(album, mean_album_ratio)) +      # reorder(album, sentiment_ratio), sentiment_ratio
  geom_col(fill = "darkgreen", alpha = 0.7) +  
  # aes(fill = sentiment_ratio > 0)   irrelevant as ALL negative...
  scale_fill_manual(guide = FALSE, values = c('#565b63', '#c40909')) +
  scale_y_percent(limits = c(-0.15, 0.10), breaks = pretty_breaks(7)) +    # from hrbrthemes
  theme_bw() +                     
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 0.95)) +
  ggtitle("Lyrics Sentiment Ratio (along album release)", 
          subtitle = "(Positive-Negative) / (Positive + Negative + Neutral), using the Bing lexicon") +
  labs(x = "Albums", y = "Sentiment Ratio (%)")

```


why negative? look at HOW negative is defined.
negative talk but with positive action with negative undertones???? 

let's see if this holds true with the AFINN lexicon, the only lexicon we haven't touched yet in the `tidytext` package!

```{r}

emotions_lyrics_afinn <- tidy_lyrics %>% 
  left_join(get_sentiments("afinn"), by = "word") 

emotions_lyrics_afinn %>% 
  summarize(NAs= sum(is.na(score)))
 
#  group_by(album) %>%   
#  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   # add in neutral
#  ungroup()


```

5226 of 6436 dont have AFINN score.... 使えない～～～～～


```{r by song}

afinn_scores <- emotions_lyrics_afinn %>% 
  replace_na(replace = list(score = 0)) %>%
  group_by(index = title) %>% 
  summarize(sentiment = sum(score)) %>% 
  mutate(lexicon = "AFINN")

bing_nrc_scores <- bind_rows(
  tidy_lyrics %>% 
    inner_join(get_sentiments("bing")) %>% 
    mutate(lexicon = "Bing"),
  tidy_lyrics %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", "negative"))) %>% 
    mutate(lexicon = "NRC")) %>% 
  # from here we count the sentiments, spread on positive/negative, then create the score:
  count(lexicon, index = title, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

all_lexicons <- bind_rows(afinn_scores, bing_nrc_scores)
lexicon_cols <- c("AFINN" = "#E41A1C", "NRC" = "#377EB8", "Bing" = "#4DAF4A")

all_lexicons %>% 
  ggplot(aes(index, sentiment, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~lexicon, ncol = 1, scales = "free_y") +
  theme_bw() +
  scale_fill_manual(values = lexicon_cols)

```

There are too many songs to be able to label them on the x-axis properly but I think you can get the general idea!

As we go through the songs from the first track on Identity Crisis, the eponymous "Identity Crisis" all the way down to the last track on last year's To Be Everywhere Is To Be Nowhere, "Salt And Shadow", the AFINN, Bing, and NRC lexicons seem to agree on the sentiments but differ in the magnitude of the expressed sentiment for a specific song.

Let's compare the lexicons on how many "positive" and "negative" words they each have.

```{r}

get_sentiments("bing") %>% 
  count(sentiment)

get_sentiments("nrc") %>% 
  count(sentiment)

```

in the BING lexicon, there are far more negatively-categorized words than positive... more than twice as many in fact...

In the Bing lexicon: there are 4782 words that can be categorized as "negative" along with 2006 "positive" words.

In the NRC lexicon: there are 3324 words that can be categorized as "negative" along with 2312 "positive" words.

```{r}

emotions_lyrics_afinn %>% 
  select(score) %>% 
  mutate(sentiment = if_else(score > 0, "positive", "negative", "NA")) %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())

emotions_lyrics_bing %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())

tidy_lyrics %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())
  
```

In all cases neutral or NA were the majority of the words...
In all the lexicons, words were categorized much more as "negative" compared to "positive". However, from PART 2 we saw that in the song, "The Weight", "won't" was used in a positive sense in ___ with phrases such as "won't abandon" or "won't leave you"; could these negation words in front of positive words be messing up our sentiment calculations? We can figure this out by separating the lyrics text into bi-grams!

Bi-grams are basically chunks of two words each in a text and we create a new dataset using the arguments in our trusty `unnest_tokens()` function!

appearance of Negation Words in THrice's lyrics!

>>> investigate bi-grams and tri-grams

## Ngrams! Splitting the text into slightly bigger word chunks!

At first I had a problem where `separate()` would ignore the apostrophes and therefore split "I'll" into "I" and "ll" which screwed up the process of splitting the bigrams into separate columns of the two individual words. 

To do this without any warning messages appearing, I had to convert all the character string vectors to ASCII format using the `iconv()` function.

For example: "I'll go" turned into `word1`: "I", `word2`: "ll", and ` `: "go" instead of `word1`: "I'll" and `word2`: "go".
After a lot of research and experimenting with different RegEx patterns I finally found a way for `separate()` to NOT delete the apostrophes and split the bigrams up properly below:

```{r}

biGrams <- df %>% 
  select(album, title, year, lyrics) %>% 
  mutate(lyrics = iconv(lyrics, to = 'latin1')) %>% 
  # convert to ASCII for better separation into ngrams
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  # split lines on the <br> tags
  unnest_tokens(ngram, line, token = "ngrams", n = 2)

biGrams_sep <- biGrams %>% 
  separate(ngram, c("word1", "word2"), sep = "[^-'\\w]")  

biGrams_sep

```

Now we have a dataframe with the bigrams separated into their individual words. Let's try counting the most common bigrams:

```{r}

biGrams_sep %>% count(word1, word2, sort = TRUE) %>% head(10)

```

bigrams of "in   the", "of   the", "we   are" dominate. Not very meaningful!

Now let's filter our the stop_words from both `word1` and `word2` and then count up again for more meaningful bigrams!

```{r}

biGrams_sep_filtered <- biGrams_sep %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word)

biGrams_count <- biGrams_sep_filtered %>% 
  count(word1, word2, sort = TRUE)

biGrams_count %>% head(10)

```








#### Negation words! How much does it screw up the sentiment scores?


```{r}

negation_words <- c("not", "no", "never", "without", "won't", "don't", "wouldn't", "couldn't")

negated_bigrams <- biGrams_sep %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word1, word2, score, sort = TRUE) %>% 
  ungroup()

negated_bigrams %>% head(10)

```

"not  alone", "no  evil", "won't leave"  are actually very positive but not counted as such.
On the other hand, "don't  care" is counted as positive!

Let's visualize this to get a clear idea of the miscalculation!

```{r}

negated_bigrams %>% 
  mutate(contribution = n * score,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>% 
  group_by(word1) %>% 
  top_n(10, abs(contribution)) %>% 
  ggplot(aes(word2, contribution, fill = n * score > 0)) +
  facet_wrap(~ word1, scales = "free") +
  geom_col(show.legend = FALSE) +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Word preceded by negation term") +
  ylab("sentiment score * number of occurences") + 
  coord_flip() +
  theme_bw()

```


how about with the BING lexicon we've been using to sum up Positive/Negative?

```{r}

negation_words <- c("not", "no", "never", "without", "won't", "don't", "wouldn't", "couldn't")

negated_bigrams <- biGrams_sep %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) %>% 
  count(word1, word2, sentiment, sort = TRUE) %>% 
  ungroup()

negated_bigrams %>% head(10)

```



Solution:

Sum up the scores for both word1 and word2 as the overall sentiment score and then visualize?


```{r}

biGrams_sep %>% 
  select(word1, word2) %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) #%>% 
  #mutate(negate_sentiment = inner_join(get_sentiments("bing"), by = c(word1 = "word")))
  





```





