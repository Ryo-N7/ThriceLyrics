---
title: "Part_3.r"
author: "Ryo Nakagawara"
date: "Fri Oct 13 21:31:45 2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r include = FALSE }
library(stringr)

library(tidyverse)
library(tidytext)
library(lubridate)

library(ggrepel)
library(scales)
library(gplots)
library(gridExtra)

library(wordcloud)

# load dataset ------------------------------------------------------------

df <- read.csv('thrice.df.csv', header = TRUE, stringsAsFactors = FALSE)

df <- df %>% 
  mutate(album = factor(album, levels = unique(album)),
         length = ms(length),
         lengthS = seconds(length))

wordToken <-  df %>%
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   
  unnest_tokens(word, line, to_lower = TRUE) 

countWord <- wordToken %>% count(word, sort=TRUE)

# Include stop_words ------------------------------------------------------

wordToken2 <- wordToken %>% 
  anti_join(stop_words) %>%                 
  arrange(ID)  # or track_num essentially same thing

countWord2 <- wordToken2 %>% count(word, sort=TRUE)

```


Let's get started!
Using the wordToken2 dataset that we created in __Part 2__ we will filter out any numbers in our data set and then use a `left_join()` function to 


```{r}
# Sentiment analysis ------------------------------------------------------

tidy_lyrics <- wordToken2 %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  group_by(album) %>%   
  mutate(linenumber = row_number(),
         sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   # add in neutral
  ungroup()

```



```{r}
# total sentiment of every word (minus stop words) from all songs
tidy_lyrics %>% 
  count(sentiment)

```

Positive: 423, Negative: 912, Neutral: 5095

# Net sentiment ratio for each song 


```{r}

tidy_lyrics %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n) %>% 
  mutate(sentiment_ratio = (positive - negative) / (positive + negative)) %>% 
  head(9)

tidy_lyrics %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  mutate(sentiment_ratio = (positive - negative) / (positive + negative)) %>% 
  head(9)

```

NAs in some sentiment columns >>> convert to ZEROs...
# use replace_na() from the tidyr package!
# or in spread() use the option fill = and set it to 0!

```{r}

tidy_lyrics %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n) %>% 
  replace_na(replace = list(negative = 0, neutral = 0, positive = 0)) %>%  # OR use fill = 0 inside spread()
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% 
  head(9)

```

# now group by album!

```{r}

tidy_lyrics %>% 
  group_by(title, album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n, fill = 0) %>% 
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) #%>% 
  #group_by(album) %>% 
  #summarize(mean_album = mean(sentiment_ratio))

```


BY ALBUM

```{r}
# Net sentiment ratio by album across time.
tidy_lyrics %>% 
  group_by(album) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n) %>%     # turn each sentiment into unique columns!!
  replace_na(replace = list(negative = 0, neutral = 0, positive = 0)) %>%
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) 

```

Now let's visualize this info!
# Lyrics sentiment ratio graph by ALBUM ####
with NEUTRAL
using BING lexicon.
geom_col() instead of geom_bar(stat = "identity") >>> basically the same thing but much better to not have to type `stat = "identity"` every single time!

```{r}
library(hrbrthemes)

tidy_lyrics %>% 
  group_by(album, year) %>% 
  count(sentiment) %>% 
  spread(key = sentiment, value = n) %>% 
  replace_na(replace = list(negative = 0, neutral = 0, positive = 0)) %>%   
  # replace NAs with ZEROs!
  mutate(sentiment_ratio = (positive - negative) / (positive + negative + neutral)) %>% # neutral 
  select(album, year, sentiment_ratio) %>% 
  ggplot(aes(album, sentiment_ratio)) +      # reorder(album, sentiment_ratio), sentiment_ratio
  geom_col(fill = "darkgreen", alpha = 0.7) +  
  # aes(fill = sentiment_ratio > 0)   irrelevant as ALL negative...
  scale_fill_manual(guide = FALSE, values = c('#565b63', '#c40909')) +
  scale_y_percent(limits = c(-0.15, 0.10), breaks = pretty_breaks(7)) +    # from hrbrthemes
  theme_bw() +                     # from hrbrthemes
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 0.95)) +
  ggtitle("Lyrics Sentiment Ratio (along album release)", 
          subtitle = "(Positive-Negative) / (Positive + Negative + Neutral)") +
  labs(x = "Albums", y = "Sentiment Ratio (%)")

```

# needs some work........    why negative? look at HOW negative is defined.
# negative talk but with positive action with negative undertones???? 

in the BING lexicon, there are far more negatively-categorized words than positive... more than twice as many in fact...

```{r}

get_sentiments("bing") %>% 
  count(sentiment)

get_sentiments("nrc") %>% 
  count(sentiment)

```

The Weight >>> showed "won't" used as positive

>>> investigate bi-grams and tri-grams





# Most common pos.neg words in THrice lyrics! ####

```{r}

word_count <- tidy_lyrics %>% 
  count(word, sentiment, sort = T) %>% 
  ungroup()

top_sentiments <-  word_count %>% 
  filter(sentiment != 'neutral') %>% 
  group_by(sentiment) %>% 
  top_n(5, wt = n) %>% 
  mutate(num = ifelse(sentiment == "negative", -n, n)) %>%  
  # count of negative words as negative #s!
  mutate(word = reorder(word, num)) %>% 
  select(word, sentiment, num)

# plot the occurences of the most common pos-neg words!

ggplot(top_sentiments, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c("black", "darkgreen")) +
  scale_y_continuous(limits = c(-40, 70), breaks = pretty_breaks(12)) + 
  # c(-40, -25, -10, 0, 10, 25, 40, 55, 70)
  labs(y = "Number of Occurrences",
       x = '',
       title = 'Lyrics Sentiment of Thrice',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))

```

# separate between negative and positive words
# order by number of occurences
# another way to visualize this is using a wordcloud!

# Word cloud: Most common Pos-Neg words in Thrice lyrics ####

```{r}
library(wordcloud)

tidy_lyrics %>% 
  filter(sentiment != 'neutral') %>% 
  count(word, sentiment, sort = T) %>% 
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("black", "darkgreen"), title.size = 1.5)

```

# LOVE appears most, free, faith, perfect, grace...
# FALL appears most, dead, burn, fear, sick...

# with nrc >>>>  Distribution of emotion words BOXPLOT: ####

```{r}

emotions_lyrics <- wordToken2 %>% 
  filter(!grepl('[0-9]', word)) %>%                  # because numbers cant have feelings
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = freq / sum(freq)) %>%   # round()   *100
  select(-freq) %>% 
  ungroup() 

emotions_lyrics %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_percent() +
  theme_bw() +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  ggtitle("Distribution of Emotion words, across all albums")

```

we can clearly see the box-plot distributions of the different emotion categories!
# black bar = mean, white circles = outliers

##### with bing >>>> only Pos-Neg-Neut ####

```{r}

emotions_lyrics_bing <- wordToken2 %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round(freq / sum(freq)*100)) %>% 
  select(-freq) %>% 
  ungroup() 

emotions_lyrics_bing %>% 
  filter(sentiment != "neutral") %>% 
  ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  xlab("Album") + ylab("Emotion Words Count (as %)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(values = c(positive = "darkgreen", negative = "black"))

```



# with NRC >>> ALL sentiments ####

# ONLY positive-negative over time/album year...   ####

```{r}

emotions_lyrics_nrc <- wordToken2 %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  filter(sentiment != "neutral") %>% 
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round(freq / sum(freq)*100)) %>% 
  select(-freq) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  ungroup() 

emotions_lyrics_nrc %>% 
  filter(sentiment == "positive" | sentiment == "negative") %>% 
  ggplot(aes(album, percent/100, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(10), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank())

```

# huge spike in positive sentiment in AI: Earth???

Quite different from using BING lexicon...

# NO pos or neg >>> anger, anticipation, disgust, fear, joy, sadness, surprise, trust

```{r}

emotions_lyrics_nrc %>% 
  filter(sentiment != "positive" & sentiment != "negative") %>% 
  ggplot(aes(album, percent/100, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(10), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_color_brewer(palette = "Set1")
  
```

# Rather messy. no noticeable trends to be found...! 
# ALthough "fear" has started to creep up after AI:Fire outlier.
# also sudden dip in "anger" in AI: Earth as seen in previous plot!

With EIGHT different emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust) our nice graph looks quite cluttered...

To solve this problem, let's split the emotions into groups to be able to see changes better!

set sentiment as factors?
>>> easier to assign to manual color scale?

NEG
anger, disgust, fear, sadness      
>>> colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)[1:4]

POS
surprise, anticipation, joy, trust 
>>> colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)[5:8]

Let's use the colors from Set1 in the RColorBrewer palettes for our emotions.

```{r}
# cols <- colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)

cols

```

Now that we've extracted the colors from Set1 let's assign them to the emotion categories.

```{r}

cols <- c("anger" = "#E41A1C", "sadness" = "#377EB8", "disgust" = "#4DAF4A", 
          "fear" = "#984EA3", "surprise" = "#FF7F00", "joy" = "#FFFF33", 
          "anticipation" = "#A65628", "trust" = "#F781BF")

emotions_lyrics_nrc %>% 
  filter(sentiment != "positive" & sentiment != "negative" &
           sentiment != "anger" & sentiment != "disgust" & 
           sentiment != "fear" & sentiment != "sadness") %>% 
  ggplot(aes(album, percent/100, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(10), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_color_manual(values = cols)

```

And now for the more negative emotions...

```{r}

emotions_lyrics_nrc %>% 
  filter(sentiment != "positive" & sentiment != "negative" &
           sentiment != "anticipation" & sentiment != "joy" & 
           sentiment != "trust" & sentiment != "surprise") %>% 
  ggplot(aes(album, percent/100, color = sentiment, group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(10), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  ggtitle("Lyric Sentiments along Albums", subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_color_manual(values = cols)

```




# AI: Earth spike in positive from large decrease in "anger", small increase in "joy"
# decrease in anticipation as well....
AI: Earth seems bereft in sentiments in general... besides joy and sadness

```{r}
# AI: Fire seems to have untrendly amount of FEAR: let's take a closer look!
nrcfear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

wordToken2 %>% 
  filter(album == "The Alchemy Index Fire") %>% 
  inner_join(nrcfear) %>% 
  count(word, sort = TRUE)

```

# FIRE being tagged as "fear" is what's mainly pushing up the trend
# fear, buried, die, gallows etc. are also in which is more in line with "fear" group

```{r}
# AI: Earth    ???
nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

wordToken2 %>% 
  filter(album == "The Alchemy Index Earth") %>% 
  inner_join(nrc_anger) %>% 
  count(word, sort = TRUE)

# appears only in 4 words!
# in other AI albums appear much more frequently!

wordToken2 %>% 
  filter(str_detect(album, "The Alchemy")) %>% 
  inner_join(nrc_anger) %>% 
  group_by(album) %>% 
  count(album, title, word, sort = TRUE)

```


## Ngrams! Splitting the text into slightly bigger word chunks!

At first I had a problem where `separate()` would ignore the apostrophes and therefore split "I'll" into "I" and "ll" which screwed up the process of splitting the bigrams into separate columns of the two individual words. 

To do this without any warning messages appearing, I had to convert all the character string vectors to ASCII format using the `iconv()` function.

For example: "I'll go" turned into `word1`: "I", `word2`: "ll", and ` `: "go" instead of `word1`: "I'll" and `word2`: "go".
After a lot of research and experimenting with different RegEx patterns I finally found a way for `separate()` to NOT delete the apostrophes and split the bigrams up properly below:

```{r}

biGrams <- df %>% 
  select(album, title, year, lyrics) %>% 
  mutate(lyrics = iconv(lyrics, to = 'latin1')) %>%                                # convert to ASCII for better separation into ngrams
  unnest_tokens(line, lyrics, token = stringr::str_split, pattern = ' <br>') %>%   # split lines on the <br> tags
  unnest_tokens(ngram, line, token = "ngrams", n = 2)

biGrams_sep <- biGrams %>% 
  separate(ngram, c("word1", "word2"), sep = "[^-'\\w]")  

biGrams_sep

```

Now we have a dataframe with the bigrams separated into their individual words. Let's try counting the most common bigrams:

```{r}

biGrams_sep %>% count(word1, word2, sort = TRUE) %>% head(10)

```

bigrams of "in   the", "of   the", "we   are" dominate. Not very meaningful!

Now let's filter our the stop_words from both `word1` and `word2`!

```{r}

biGrams_sep_filtered <- biGrams_sep %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word)

```

Now let's try counting again for more meaningful bigrams!

```{r}

biGrams_count <- biGrams_sep_filtered %>% 
  count(word1, word2, sort = TRUE)

biGrams_count %>% head(10)

```



```{r}

biGrams_tfidf <- biGrams_sep_filtered %>%
  unite(ngram, word1, word2, sep = " ") %>% 
  count(album, ngram) %>% 
  bind_tf_idf(ngram, album, n) %>% 
  arrange(desc(tf_idf))

biGrams_tfidf

```

Skewed toward the smaller albums? AI each has 6 songs each compared to ~12 for others. Consider that AI are actually split into two albums of two elements each.


```{r}

biGrams_tfidf %>% 
  group_by(album) %>% 
  top_n(3) %>% 
  ungroup() %>% 
  ggplot(aes(reorder(ngram, tf_idf), tf_idf, fill = album)) + 
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~album, ncol = 3, scales = "free") +
  coord_flip()

```


#### Negation words! How much does it screw up the sentiment scores?














```{r}
# Comparison Alchemy Index albums! ----------------------------------------

# Fire vs. Water
# Earth vs. Air

alchemy_index <- wordToken2 %>% 
  filter(str_detect(album, "The Alchemy Index")) %>% 
  select(title, album, year, word)

alchemy_index %>% 
  left_join(get_sentiments("nrc"), "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment))

# to not clutter up global environment...
tidy_lyrics %>% 
  filter(str_detect(album, "The Alchemy Index")) %>% 
  select(title, album, year, word) %>% 
  left_join(get_sentiments("nrc"), "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>% 
  filter(sentiment != "neutral") %>% 
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round(freq / sum(freq)*100)) %>% 
  select(-freq) %>% 
  ungroup() %>% 
  head(20)

# with neutral
tidy_lyrics %>% 
  filter(str_detect(album, "The Alchemy Index")) %>% 
  select(title, album, year, word) %>% 
  left_join(get_sentiments("nrc"), "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>% 
  group_by(album, sentiment) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round(freq / sum(freq)*100)) %>% 
  select(-freq) %>% 
  ungroup() %>% 
  filter(sentiment != "neutral") %>% 
  filter(sentiment != "positive") %>% 
  filter(sentiment != "negative") %>% 
  ggplot(aes(x = album, y = percent/100, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(10), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Emotion Words") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank())
# not informative... no timeline order of albums Fire or Water first? 
# ordering changes trends dramatically

```


